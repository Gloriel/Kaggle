{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–µ—Ä–µ–≤—å—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "===================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* –ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏<br>\n",
    "  * [–±—ç–≥–≥–∏–Ω–≥, —Å–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å](https://habr.com/ru/company/ods/blog/324402/).<br>\n",
    "  * [–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥](https://habr.com/ru/company/ods/blog/327250/).<br>\n",
    "  * [–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è, –¥–µ—Ä–µ–≤—å—è —Ä–µ—à–µ–Ω–∏–π –∏ –º–µ—Ç–æ–¥ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π](https://habr.com/ru/company/ods/blog/322534/).<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from pylab import plot,show,hist\n",
    "from scipy.stats.kde import gaussian_kde\n",
    "from scipy.stats import norm, chi2_contingency\n",
    "import statsmodels.api as sm\n",
    "from numpy import linspace,hstack\n",
    "from pylab import plot,show,hist\n",
    "import pydot\n",
    "#%config InlineBackend.figure_format = 'svg' –¥–ª—è –±–æ–ª—å—à–µ–π —á–µ—Ç–∫–æ—Å—Ç–∏ –≥—Ä–∞—Ñ–∏–∫–æ–≤\n",
    "matplotlib.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "#–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –¥–∏–∞–≥—Ä–∞–º–º —Ä–∞—Å—Å–µ–∏–≤–∞–Ω–∏—è\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "#–ì—Ä–∞—Ñ–∏–∫–∞ –¥–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π\n",
    "from IPython.display import Image\n",
    "from sklearn.tree import export_graphviz\n",
    "from subprocess import call\n",
    "\n",
    "#–î–µ—Ä–µ–≤—å—è —Ä–µ—à–µ–Ω–∏–π –¥–ª—è –∑–∞–¥–∞—á–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#–î–µ—Ä–µ–≤—å—è —Ä–µ—à–µ–Ω–∏–π –¥–ª—è –∑–∞–¥–∞—á —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#–ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –¥–µ—Ä–µ–≤—å–µ–≤ —Ä–µ—à–µ–Ω–∏–π\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "os.chdir(r'C:\\Users\\Mr Alex\\Documents\\GitHub\\FlightPreparence')\n",
    "columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "           'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "           'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "\n",
    "\n",
    "#df = pd.read_csv('AmesHousing.txt', sep=\"\\t\", header = 0, index_col=False)\n",
    "#df = pd.read_csv('town_1959_2.csv', header = 0,)\n",
    "#df = pd.read_csv('swiss_bank_notes.csv', index_col=0)\n",
    "#df = pd.read_csv('beverage_r.csv', sep=\";\", index_col='numb.obs')\n",
    "#df = pd.read_csv('Protein Consumption in Europe.csv', sep=';', decimal=',', index_col='Country')\n",
    "#df = pd.read_csv('assess.dat', sep='\\t', index_col='NAME')\n",
    "#df = pd.read_csv('Albuquerque Home Prices_data.txt', sep='\\t')\n",
    "#df = pd.read_csv('agedeath.dat.txt', sep='\\s+', header=None, names=['group', 'age', 'index'])\n",
    "#df = pd.read_csv('interference.csv')\n",
    "#df = pd.read_csv('diamond.dat', header=None, sep='\\s+', names=['weight', 'price'])\n",
    "#df = pd.read_csv('Credit.csv', sep=';', encoding='cp1251')\n",
    "#df = pd.read_csv('adult.test', header=None, names=columns, na_values=' ?', skiprows=1)\n",
    "#df = pd.read_csv('Wine.txt', sep='\\t', header=0)\n",
    "#df = pd.read_csv('monthly-car-sales-in-quebec-1960.csv', sep=';', header=0, parse_dates=[0])\n",
    "#df = pd.read_csv('stickleback.csv', sep=';', decimal=',')\n",
    "df = pd.read_csv('Swiss Fertility.csv', sep=';', decimal=',', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_histograms(x, y):\n",
    "    \"\"\"\n",
    "    –§—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è –ø–æ—Å—Ç—Ä–æ–∏—Ç –¥–≤–µ –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã –Ω–∞ –æ–¥–Ω–æ–π –∫–∞—Ä—Ç–∏–Ω–∫–µ.\n",
    "    –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø—É–Ω–∫—Ç–∏—Ä–Ω—ã–º–∏ –ª–∏–Ω–∏—è–º–∏ —É–∫–∞–∑—ã–≤–∞—é—Ç—Å—è —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤—ã–±–æ—Ä–æ–∫.\n",
    "    x: –≤–µ–∫—Ç–æ—Ä pd.Series,\n",
    "    y: –≤–µ–∫—Ç–æ—Ä pd.Series\n",
    "    \"\"\"\n",
    "    x.hist(alpha=0.5, weights=[1./len(x)]*len(x))\n",
    "    y.hist(alpha=0.5, weights=[1./len(y)]*len(y))\n",
    "    plt.axvline(x.mean(), color='red', alpha=0.8, linestyle='dashed')\n",
    "    plt.axvline(y.mean(), color='blue', alpha=0.8, linestyle='dashed')\n",
    "    plt.legend([x.name, y.name])\n",
    "    \n",
    "def regression_coef(model, X, y):\n",
    "    \"\"\"\n",
    "    –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã—Ö –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤\n",
    "    \"\"\"\n",
    "    coef = pd.DataFrame(zip(['intercept'] + X.columns.tolist(), [model.intercept_] + model.coef_.tolist()),\n",
    "                    columns=['predictor', 'coef'])\n",
    "    X1 = np.append(np.ones((len(X),1)), X, axis=1)\n",
    "    b = np.append(model.intercept_, model.coef_)\n",
    "    MSE = np.sum((model.predict(X) - y) ** 2, axis=0) / float(X.shape[0] - X.shape[1])\n",
    "    var_b = MSE * (np.linalg.inv(np.dot(X1.T, X1)).diagonal())\n",
    "    sd_b = np.sqrt(var_b)\n",
    "    t = b / sd_b\n",
    "    coef['pvalue'] = [2 * (1 - stats.t.cdf(np.abs(i), (len(X1) - 1))) for i in t]\n",
    "    return coef\n",
    "\n",
    "\n",
    "def scale_features(df):\n",
    "    \"\"\"\n",
    "    –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ\n",
    "    \"\"\"\n",
    "    scaled = preprocessing.StandardScaler().fit_transform(df)\n",
    "    scaled = pd.DataFrame(scaled, columns=df.columns)\n",
    "    return scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–¥–µ–ª–∞—Ç—å –≤–µ—Å –≤–∞–∂–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å–æ–∏–∑–º–µ—Ä–∏–º—ã–º. Min=0(-1), max=1. –ò–õ–ò Z\n",
    "#Z-–°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è: –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ç–∏–ø, –≥–¥–µ –ú=0, sd = 1. –ü—Ä–∞–≤–∏–ª–æ –æ–¥–Ω–æ–π, –¥–≤—É—Ö –∏ —Ç—Ä–µ—Ö \"—Å–∏–≥–º\"\n",
    "#Z=(X–∏–Ω–¥-–ú)/sd –ü—Ä–∏–º–µ—Ä: –ø–æ —Ç–∞–±–ª–∏—Ü–µ Z, –≥–¥–µ –•—Å—Ä–µ–¥=150, sd=8, –ø—Ä–µ–≤—ã—à–∞—Ç—å X–∏–Ω–¥ –±—É–¥–µ—Ç 0.5z –∏–ª–∏ 30%\n",
    "#Z=(X—Å—Ä–µ–¥-M)/se =(18,5-20)/0.5 = -3. –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–æ–ª—É—á–∏—Ç—å —Ç–∞–∫–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç p = 0.0027\n",
    "\n",
    "#–ï—Å–ª–∏ –≤ –ë–î –Ω–µ—Ç –µ–¥–∏–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏, —Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "norm = preprocessing.StandardScaler()\n",
    "norm.fit(df)\n",
    "X = norm.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#–ó–∞–¥–∞—á–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–π –∏–ª–∏ –ø–æ—Ä—è–¥–∫–æ–≤ —á–µ—Ä–µ–∑ –¥–µ—Ä–µ–≤—å—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏. –ò —á–∏—Å–µ–ª —á–µ—Ä–µ–∑ —Ä–µ–≥—Ä–µ—Å—Å–∏—é\n",
    "#–ü–æ–º–∏–º–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–∑–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ), –µ—Å—Ç—å –µ—â–µ –≤–Ω–µ—à–Ω–∏–µ (–∑–∞–¥–∞–≤–∞–µ–º—ã–µ –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–º)\n",
    "#–í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ —Å –ø–æ–º–æ—â—å—é –æ–±—É—á–∞—é—â–µ–π/—Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–æ–∫ —á–µ—Ä–µ–∑ –Ω–∞–∏–º–µ–Ω—å—à—É—é —Å—Ä–µ–¥–Ω—é—é –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω—É—é –æ—à–∏–±–∫—É \n",
    "#–ö—Ä–∏—Ç–µ—Ä–∏–π –∫–∞—á–µ—Å—Ç–≤–∞ Q - —Å—É–º–º–∞ –º–æ–¥—É–ª–µ–π –æ—à–∏–±–æ–∫ –∏–ª–∏ —Å—É–º–º–∞ –∫–≤–∞–¥—Ä–∞—Ç–æ–≤ –æ—à–∏–±–æ–∫ –∏–ª–∏ –ø—Ä–æ—Ü–µ–Ω—Ç –æ—à–∏–±–æ–∫ –∏ —Ç.–¥.\n",
    "#–í–∞–ª–∏–¥–∞—Ü–∏—è - –º–µ—Ç–æ–¥ –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –µ–µ –∞–¥–µ–∫–≤–∞—Ç–Ω–æ—Å—Ç—å\n",
    "#–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è - –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ –º–æ–¥–µ–ª–µ–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CART - Classification and regression trees\n",
    "#–¥–µ–ª–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –ø—Ä—è–º—ã–º–∏\\–≥–∏–ø–µ—Ä–ø–ª–æ—Å–∫–æ—Å—Ç—è–º–∏, —á—Ç–æ–±—ã –≤ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö –¥–æ–º–∏–Ω–∏—Ä–æ–≤–∞–ª–∏ —Å—Ö–æ–∂–∏–µ –æ–±—ä–µ–∫—Ç—ã\n",
    "#–£–∑–µ–ª(node) - –º–Ω–æ–∂–µ—Å—Ç–≤–æ, –∫–æ—Ç–æ—Ä–æ–µ —Ä–∞—Å—â–µ–ø–ª—è–µ—Ç—Å—è. –†–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π, –ø–æ—Ç–æ–º–æ–∫, –∫–æ–Ω–µ—á–Ω—ã–π. \n",
    "#–ü–æ—Ä–æ–≥–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ - —ç—Ç–∞–ª–æ–Ω –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
    "#–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∑–∞–¥–∞—é—Ç—Å—è –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–º. –ù–∞ –∫–æ–ª-–≤–æ —Å–ª–æ–µ–≤, –Ω–∞ —Å–≤–æ–π—Å—Ç–≤–æ –ø–æ—Ç–æ–º–∫–æ–≤, –Ω–∞ —Ä–æ–¥–∏—Ç–µ–ª—è, –Ω–∞ –ø—Ä–∞–≤–∏–ª–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ \n",
    "#–ß–∏—Å—Ç–æ—Ç–∞ - –ø–æ—Ä—è–¥–æ–∫ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –≤—ã–±–æ—Ä–∫–∏ –Ω–∞ —á–∞—Å—Ç–∏, –≤ –∫–∞–∂–¥–æ–π –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö \"–∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏–µ\" –¥–∞–Ω–Ω—ã—Ö –º–µ–Ω—å—à–µ\n",
    "#–ö—Ä–∏—Ç–µ—Ä–∏–π –∑–∞–≥—Ä–∞–∑–Ω–µ–Ω–Ω–æ—Å—Ç–∏(–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∞—Ç—å –∫ –∫–ª–∞—Å—Å—É P) –∏–∑–º–µ—Ä—è–µ—Ç—Å—è –≠–Ω—Ç—Ä–æ–ø–∏–µ–π, –ò–Ω–¥–µ–∫—Å–æ–º –î–∂–∏–Ω–∏ –∏–ª–∏ –û—à–∏–±–∫–æ–π –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "#–≠–Ω—Ç—Ä–æ–ø–∏—è H1 = -–°—É–º–º–∞P*log2P. –ò–Ω–¥–µ–∫—Å –î–∂–∏–Ω–∏ H2 = 1-–°—É–º–º–∞P**2 = –°—É–º–º–∞P*(1-P). –û—à–∏–±–∫–∞ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ H3 = 1-maxP\n",
    "#–î–µ–ª—å—Ç–∞ H - –≤–∫–ª–∞–¥ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –≤ –æ—á–∏—â–µ–Ω–∏–µ. –°—á–∏—Ç–∞–µ–º —Å—É–º–º—ã –¥–ª—è –∫–∞–∂–¥–æ–π –∏ –ø–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#–†–∞—Å—â–µ–ø–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏\n",
    "\n",
    "X = df.drop('Survived', axis=1)\n",
    "y = df['Survived']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42,\n",
    "                                                    # –¥–æ–ª—è –æ–±—ä—ë–º–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –º–Ω–æ–∂–µ—Å—Ç–≤–∞\n",
    "                                                    test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#–ó–∞–¥–∞—á–∞ –∫—Ä–µ–¥–∏—Ç–Ω–æ–≥–æ —Å–∫–æ—Ä–∏–Ω–≥–∞\n",
    "# –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ –≤–µ–∫—Ç–æ—Ä y\n",
    "y = df[u'–∫—Ä–µ–¥–∏—Ç']\n",
    "# –£–¥–∞–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –æ—Ç–≤–µ—Ç–æ–º\n",
    "X = df.drop(u'–∫—Ä–µ–¥–∏—Ç', axis=1)\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∏ –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å\n",
    "model = DecisionTreeClassifier(random_state=42,\n",
    "                               # —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è impurity ('gini' –∏–ª–∏ 'entropy')\n",
    "                               criterion='gini',\n",
    "                               # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤–∞\n",
    "                               max_depth=5,\n",
    "                               # –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ —É–∑–ª–µ –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è (–º–æ–∂–µ—Ç –±—ã—Ç—å –¥–æ–ª–µ–π)\n",
    "                               min_samples_split=5,\n",
    "                               # –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –≤ –ª–∏—Å—Ç–µ (–º–æ–∂–µ—Ç –±—ã—Ç—å –¥–æ–ª–µ–π)\n",
    "                               min_samples_leaf=5,\n",
    "                               # –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–µ–ª—å—Ç—ã impurity\n",
    "                               # min_impurity_decrease=0,\n",
    "                               # –≤–µ—Å–∞ –¥–ª—è –∫–ª–∞—Å—Å–æ–≤ (–º–æ–∂–Ω–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —à—Ç—Ä–∞—Ñ–æ–≤–∞—Ç—å –∑–∞ –æ—à–∏–±–∫—É –≤ –Ω—É–∂–Ω—ã—Ö –∫–ª–∞—Å—Å–∞—Ö).\n",
    "                               # –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ–ø—Ü–∏—é 'balanced'.\n",
    "                               class_weight=None\n",
    "                               \n",
    "                              )\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "#–î–ª—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ –ø–æ–ª—É—á–∏–≤—à–µ–π—Å—è –º–æ–¥–µ–ª–∏ –∏–∑–æ–±—Ä–∞–∂–∞–µ–º –µ—ë –≤ –≤–∏–¥–µ –¥–µ—Ä–µ–≤–∞ –ø—Ä–µ–¥–∏–∫–∞—Ç–æ–≤ (—Ä–µ—à–∞—é—â–∏—Ö –ø—Ä–∞–≤–∏–ª)\n",
    "\n",
    "export_graphviz(model,\n",
    "                out_file='tree.dot',\n",
    "                #–∑–∞–¥–∞—Ç—å –Ω–∞–∑–≤–∞–Ω–∏—è —Ñ–∏—á\n",
    "                #feature_names=X.columns,\n",
    "                class_names=None,\n",
    "                #–ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –Ω–∞–∑–≤–∞–Ω–∏—è –ø–æ–ª–µ–π —É —á–∏—Å–ª–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤–Ω—É—Ç—Ä–∏ —É–∑–ª–∞\n",
    "                label='all',\n",
    "                #—Ä–∞—Å–∫—Ä–∞—à–∏–≤–∞—Ç—å —É–∑–ª—ã –≤ —Ü–≤–µ—Ç –ø—Ä–µ–æ–±–ª–∞–¥–∞—é—â–µ–≥–æ –∫–ª–∞—Å—Å–∞\n",
    "                filled=True,\n",
    "                #–ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ impurity –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —É–∑–ª–∞\n",
    "                impurity=True,\n",
    "                #–ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –Ω–æ–º–µ—Ä–∞ —É–∑–ª–æ–≤\n",
    "                node_ids=True,\n",
    "                #–ü–æ–∫–∞–∑—ã–≤–∞—Ç—å –¥–æ–ª–∏ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞ –≤ —É–∑–ª–∞—Ö (–∞ –Ω–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ)\n",
    "                proportion=True,\n",
    "                #–ü–æ–≤–µ—Ä–Ω—É—Ç—å –¥–µ—Ä–µ–≤–æ –Ω–∞ 90 –≥—Ä–∞–¥—É—Å–æ–≤ (–≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–∞—è –æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏—è)\n",
    "                rotate=True,\n",
    "                #–ß–∏—Å–ª–æ —Ç–æ—á–µ–∫ –ø–æ—Å–ª–µ –∑–∞–ø—è—Ç–æ–π –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º—ã—Ö –¥—Ä–æ–±–µ–π\n",
    "                #precision=3\n",
    "               )\n",
    "\n",
    "#–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ñ–∞–π–ª .dot –≤ .png\n",
    "#node - –Ω–æ–º–µ—Ä —É–∑–ª–∞, X[1]<=1.5 –ø—Ä–∞–≤–∏–ª–æ —Ä–∞—Å—â–µ–ø–ª–µ–Ω–∏—è, gini, samples-–¥–æ–ª—è –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –ø–æ–ø–∞–≤—à–∏—Ö –≤ —É–∑–µ–ª, p-value (p0, pX)\n",
    "(graph,) = pydot.graph_from_dot_file('tree.dot')\n",
    "graph.write_png('tree.png')\n",
    "Image(\"tree.png\")\n",
    "\n",
    "#–ú–æ–¥–µ–ª—å –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ü–µ–Ω–∏—Ç—å —Ü–µ–Ω–Ω–æ—Å—Ç—å (importance) –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–π —Ñ–∏—á–∏, —Å—á–∏—Ç–∞—è –¥–ª—è –∫–∞–∂–¥–æ–π –∏–∑ —Å—É–º–º—É –¥–µ–ª—å—Ç—ã H \n",
    "pd.DataFrame({'feature': X.columns,\n",
    "              'importance': model.feature_importances_}).sort_values('importance', \n",
    "            ascending=False\n",
    "            )\n",
    "\n",
    "#–ú–µ—Ç–æ–¥ predict –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ (–ø–æ–¥–∞—ë–º –Ω–∞ –≤—Ö–æ–¥ –º–∞—Ç—Ä–∏—Ü—É)\n",
    "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∞ –¥–ª—è –Ω–æ–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤\n",
    "new_item = [1, 1, 1, 1]\n",
    "model.predict([new_item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#–†–∞—Å—â–µ–ø–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—ã–µ –≤—ã–±–æ—Ä–∫–∏\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42,\n",
    "                                                    # –¥–æ–ª—è –æ–±—ä—ë–º–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –º–Ω–æ–∂–µ—Å—Ç–≤–∞\n",
    "                                                    test_size=0.2)\n",
    "#–æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#–°—Ç—Ä–æ–∏–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞: –¥–æ–ª—è —Å–æ–≤–ø–∞–≤—à–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤ –≤ y_pred –∏ y_test, –∏–ª–∏ —Å—á–∏—Ç–∞–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å –∏ –ø–æ–ª–Ω–æ—Ç—É\n",
    "#–ï—Å–ª–∏ –¥–æ–ª—è –≤ –æ–±—É—á–∞—é—â–µ–º –≤—ã—à–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ, –æ–∑–Ω–∞—á–∞–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏. –ù—É–∂–Ω–æ —É–ø—Ä–æ—â–∞—Ç—å –º–æ–¥–µ–ª—å\n",
    "#–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫  ùê∂=(ùëêùëñ,ùëó) , –≥–¥–µ  ùëêùëñ,ùëó –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –∫–ª–∞—Å—Å–∞ ùëñ , –∫–æ—Ç–æ—Ä—ã–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø—Ä–∏—Å–≤–æ–∏–ª –∫–ª–∞—Å—Å ùëó \n",
    "#–¢–æ—á–Ω–æ—Å—Ç—å(precision) - –¥–æ–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–º. \n",
    "#–ü–æ–ª–Ω–æ—Ç–∞(recall) - –¥–æ–ª—è —ç—Ç–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –ù–ê –°–ê–ú–û–ú –î–ï–õ–ï\n",
    "conf_mat = metrics.confusion_matrix(y_test, y_pred)\n",
    "conf_mat = pd.DataFrame(conf_mat, index=model.classes_, columns=model.classes_)\n",
    "conf_mat\n",
    "\n",
    "#–ì–∞—Ä–º–æ–Ω–∏—á–µ—Å–∫–æ–µ —Å—Ä–µ–¥–Ω–µ–µ F1 = 2*—Ç–æ—á–Ω–æ—Å—Ç—å*–ø–æ–ª–Ω–æ—Ç–∞/(—Ç–æ—á–Ω–æ—Å—Ç—å+–ø–æ–ª–Ω–æ—Ç–∞). –°—á–∏—Ç–∞–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é classification_report\n",
    "print(metrics.classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#–î–µ—Ä–µ–≤—å—è —Ä–µ—à–µ–Ω–∏–π –¥–ª—è –∑–∞–¥–∞—á —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ (–æ—Ç–∫–ª–∏–∫ –Ω–µ –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–π, –∞ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–π). –ú–µ—Ç–æ–¥—ã —Å—Ö–æ–∂—ã —Å –¥–µ—Ä–µ–≤–æ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "#–ü—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–∏–∏ –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏, –∫–æ–≥–¥–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –Ω–µ –ª–∏–Ω–µ–π–Ω–∞—è :)\n",
    "#–í —ç—Ç–æ–º —Å–ª—É—á–∞ –î–µ–ª—å—Ç–∞ H = —Å—É–º–º–∞ –∫–≤–∞–¥—Ä–∞—Ç–æ–≤ –æ—à–∏–±–æ–∫\n",
    "#Prune (–æ–±—Ä–µ–∑–∞–Ω–∏–µ) - –æ—á–∏—Å—Ç–∫–∞ –æ—Ç —É–∑–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –Ω—É–∂–Ω—ã, —á–µ—Ä–µ–∑ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç—Ä–µ—Ç—å–µ–π –≤—ã–±–æ—Ä–∫–∏ (–≤–∞–ª–∏–¥–∞—Ü–∏–∏)\n",
    "\n",
    "#–°–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å. –ö–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
    "#ntree - —á–∏—Å–ª–æ –¥–µ—Ä–µ–≤—å–µ–≤(–≤ –Ω–∞—á–∞–ª–µ –º–∞–∫—Å, –ø–æ—Ç–æ–º —Å–æ–∫—Ä–∞—â–∞—Ç—å), mtry - —á–∏—Å–ª–æ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –≤ –≤—ã–±–æ—Ä–∫–µ (M**0.5)\n",
    "#sampsize - —á–∏—Å–ª–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –≤ –ø–æ–¥–≤—ã–±–æ—Ä–∫–µ(0.632*N –¥–ª—è –¥–µ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏), nodesize - –º–∏–Ω. —á–∏—Å–ª–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –≤ —É–∑–ª–µ (10) \n",
    "#replace - –ø–æ–¥–≤—ã–±–æ—Ä–∫–∞ —Å  –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ–º –∏–ª–∏ –±–µ–∑\n",
    "#out-of-bag - –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —á–∞—Å—Ç—å –≤—ã–±–æ—Ä–∫–∏, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –∫–∞—á–µ—Å—Ç–≤–µ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞ –º–æ–¥–µ–ª–∏\n",
    "\n",
    "#–ï—Å–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–ª–∞—Å—Å–æ–≤, –Ω–æ —Ö–æ—á–µ—Ç—Å—è —Å–¥–µ–ª–∞—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é —Å—Ç—Ä–æ–≥–æ –±–∏–Ω–∞—Ä–Ω–æ–π, —Ç–æ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –≥—Ä—É–ø–ø—ã –î–ê –∏ –ù–ï–¢\n",
    "df['Desired1(3)'] = df['Desired1(3)'].replace(0, 1)\n",
    "\n",
    "\n",
    "model = RandomForestClassifier(random_state=42, #–∑–µ—Ä–Ω–æ –¥–∞—Ç—á–∏–∫–∞ —Å–ª—É—á–∞–π–Ω—ã—Ö —á–∏—Å–µ–ª\n",
    "                               #—á–∏—Å–ª–æ –¥–µ—Ä–µ–≤—å–µ–≤ –≤ –ª–µ—Å—É\n",
    "                               n_estimators=30,\n",
    "                               #—Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –¥–µ–ª—å—Ç–∞ H, impurity ('gini' –∏–ª–∏ 'entropy')\n",
    "                               criterion='gini',\n",
    "                               #–ú–∞–∫—Å —á–∏—Å–ª–æ —Å–ª–æ–µ–≤\n",
    "                               max_depth=5,\n",
    "                               #–í—ã—á–∏—Å–ª—è—Ç—å out-of-bag –æ—à–∏–±–∫—É\n",
    "                               oob_score=True,\n",
    "                               #–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –≤—ã–∑–æ–≤–∞ –∏ –Ω–∞—Ä–∞—Å—Ç–∏—Ç—å –ø—Ä–µ–¥—ã–¥—É—â–∏–π –ª–µ—Å \n",
    "                               warm_start=False,\n",
    "                               #–≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "                               class_weight=None\n",
    "                               \n",
    "                              )\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(metrics.classification_report(y_pred, y_test))\n",
    "\n",
    "print('Out-of-bag score: {0}'.format(model.oob_score_)) \n",
    "\n",
    "pd.DataFrame({'feature': X.columns,\n",
    "              'importance': model.feature_importances_}).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#–í–∞—Ä–∏–∞–Ω—Ç 2 –°–ª—É—á–∞–π–Ω–æ–≥–æ –ª–µ—Å–∞\n",
    "\n",
    "#–î–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\n",
    "X_to_be_predicted = df[df.Data.isnull()]\n",
    "X_to_be_predicted = X_to_be_predicted.drop(['data'], axis = 1)\n",
    "# X_to_be_predicted[X_to_be_predicted.Age.isnull()]\n",
    "# X_to_be_predicted.dropna(inplace = True) # 417 x 27\n",
    "#Training data\n",
    "train_data = df\n",
    "train_data = train_data.dropna()\n",
    "feature_train = train_data['data']\n",
    "label_train = train_data.drop(['data'], axis = 1)\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(criterion='entropy',\n",
    "                                n_estimators=700,\n",
    "                                min_samples_split=10,\n",
    "                                min_samples_leaf=1,\n",
    "                                max_features='auto',\n",
    "                                oob_score=True,\n",
    "                                random_state=1,\n",
    "                                n_jobs=-1\n",
    "                            )\n",
    "x_train, x_test, y_train, y_test = train_test_split(label_train, feature_train, test_size=0.2)\n",
    "\n",
    "clf.fit(x_train, np.ravel(y_train))\n",
    "\n",
    "print(\"RF Accuracy: \"+repr(round(clf.score(x_test, y_test) * 100, 2)) + \"%\")\n",
    "\n",
    "result_rf=cross_val_score(clf,x_train,y_train,cv=10,scoring='accuracy')\n",
    "\n",
    "print('The cross validated score for Random forest is:',round(result_rf.mean()*100,2))\n",
    "\n",
    "y_pred = cross_val_predict(clf,x_train,y_train,cv=10)\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_train,y_pred),annot=True,fmt='3.0f',cmap=\"summer\")\n",
    "\n",
    "plt.title('Confusion_matrix for RF', y=1.05, size=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#–ü—Ä–∏–µ–º—ã —É–ª—É—á—à–µ–Ω–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤: stacking, bagging, boosting\n",
    "#Stacking(–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –±–∞–∑–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π)\n",
    "#Bagging(—É—Å—Ä–µ–¥–Ω–µ–Ω–Ω–æ–µ –º–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π), –æ–Ω –∂–µ —Å–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å. –ß—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –∫–æ–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏, –≤—ã–±–æ—Ä–∫–∏ —Å–æ–±–∏—Ä–∞—é—Ç—Å—è —Ä–∞–Ω–¥–æ–º–Ω–æ\n",
    "#Boosting - –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—à–∏–±–æ–∫ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ (—É–ª—É—á—à–µ–Ω–∏–µ–º —Å–ª–∞–±–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GBM - Gradient Boosting Machine. –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –±—É—Å—Ç–∏–Ω–≥–∞, –∫–æ–≥–¥–∞ –æ—á–µ—Ä–µ–¥–Ω—ã–µ —Ü–∏–∫–ª—ã –ø–µ—Ä—Å—Ç–∞—é—Ç —É–ª—É—á—à–∞—Ç—å –º–æ–¥–µ–ª—å\n",
    "#–°—É–º–º–∞ –∫–≤–∞–¥—Ä–∞—Ç–æ–≤ –æ—à–∏–±–æ–∫ Zi = -2*(Yi - f(Xi))\n",
    "#–ú–µ—Ç–æ–¥ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏—è. –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω–æ–µ —Å–æ–±—ã—Ç–∏–µ. –ö—Ä–∏—Ç–µ—Ä–∏–π –∫–∞—á–µ—Å—Ç–≤–∞ = P**A*(1-P)**(n-A)\n",
    "#–ö—Ä–∏—Ç–µ—Ä–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ì–∞—É—Å—Å–∞ –∏ –õ–∞–ø–ª–∞—Å–∞ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã. –î–ª—è –¥–≤—É—Ö –∫–ª–∞—Å—Å–æ–≤ - –±–∏–Ω–æ–º–∏–∞–ª—å–Ω–æ–µ, –¥–ª—è –±–æ–ª—å—à–µ–≥–æ - –º—É–ª—å—Ç–∏–Ω–æ–º–∏–Ω–∞–ª—å–Ω–æ–µ\n",
    "#–ü—Ä–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–º –ø—Ä–æ–º–µ–∂—É—Ç–∫–µ - —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ü—É–∞—Å—Å–æ–Ω–∞\n",
    "\n",
    "columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "           'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "           'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "df = pd.read_csv('adult.data', header=None, names=columns, na_values=' ?')\n",
    "\n",
    "# –£–¥–∞–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É education (–ø–æ—Ç–æ–º—É —á—Ç–æ –µ—Å—Ç—å —É–∂–µ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–æ–ª–æ–Ω–∫–∞ education-num)\n",
    "df = df.drop('education', axis=1)\n",
    "\n",
    "# –ö–æ–¥–∏—Ä—É–µ–º –æ—Ç–∫–ª–∏–∫ –≤ –±–∏–Ω–∞—Ä–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "df['income'] = df['income'].map({' <=50K': 0, ' >50K': 1})\n",
    "\n",
    "# —É–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NA –∑–Ω–∞—á–µ–Ω–∏—è–º–∏\n",
    "df = df.dropna()\n",
    "\n",
    "test = pd.read_csv('adult.test', header=None, names=columns, na_values=' ?', skiprows=1)\n",
    "test = test.drop('education', axis=1)\n",
    "test['income'] = test['income'].map({' <=50K.': 0, ' >50K.': 1})\n",
    "test = test.dropna()\n",
    "\n",
    "#–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ –æ—Ç–∫–ª–∏–∫–µ\n",
    "df['income'].value_counts(normalize=True)\n",
    "\n",
    "#–†–∞–∑–±–∏–≤–∞–µ–º –¥–∞—Ç—É. –ë–∏–Ω–∞—Ä–∏–∑—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (one-hot encoding).\n",
    "X_train = pd.get_dummies(df).drop('income', axis=1)\n",
    "y_train = df['income']\n",
    "\n",
    "X_test = pd.get_dummies(test).drop('income', axis=1)\n",
    "y_test = test['income']\n",
    "\n",
    "#–í —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç –æ–¥–Ω–æ–π –∫–æ–ª–æ–Ω–∫–∏ \n",
    "print(len(X_train.columns))\n",
    "print(len(X_test.columns))\n",
    "\n",
    "#–ü—Ä–∏–≤–æ–¥–∏–º –º–Ω–æ–∂–µ—Å—Ç–≤–æ –Ω–∞–∑–≤–∞–Ω–∏–π –∫–æ–ª–æ–Ω–æ–∫ –∫ —Ç–∏–ø—É set, –Ω–∞—Ö–æ–¥–∏–º —Ä–∞–∑–Ω–æ—Å—Ç—å –¥–≤—É—Ö –º–Ω–æ–∂–µ—Å—Ç–≤: –ì–æ–ª–ª–∞–Ω–¥–∏–∏ –Ω–µ—Ç –≤ –∫–æ–ª–æ–Ω–∫–µ native-county \n",
    "print(set(X_train.columns) - set(X_test.columns))\n",
    "print(set(X_test.columns) - set(X_train.columns))\n",
    "\n",
    "#–î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â—É—é –∫–æ–ª–æ–Ω–∫—É. –°–º–æ—Ç—Ä–∏–º, —Å—Ç–æ–∏—Ç –ª–∏ —Å–∫–ª–µ–∏–≤–∞—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –≤ –±–æ–ª–µ–µ –∫—Ä—É–ø–Ω—ã–µ –∫–ª–∞—Å—Å—ã\n",
    "columns = set(X_train.columns) | set(X_test.columns)\n",
    "X_train = X_train.reindex(columns=columns).fillna(0)\n",
    "X_test = X_test.reindex(columns=columns).fillna(0)\n",
    "\n",
    "#–ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫ (–µ—Å–ª–∏ –¥–∞, —Ç–æ True)\n",
    "all(X_train.columns == X_test.columns)\n",
    "\n",
    "#–°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å\n",
    "model = GradientBoostingClassifier(random_state=42,\n",
    "                                   # –ß–∏—Å–ª–æ –¥–µ—Ä–µ–≤—å–µ–≤\n",
    "                                   n_estimators=500,\n",
    "                                   #–∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏–µ –∏–∑–º–µ—Ä—è–µ–º ‚Äúmse‚Äù, ‚Äúmae‚Äù –∏–ª–∏ ‚Äúfriedman_mse‚Äù (mse —Å —É–ª—É—á—à–µ–Ω–∏—è–º–∏)  \n",
    "                                   criterion='friedman_mse', \n",
    "                                   #–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ä–µ–≤–∞\n",
    "                                   #–∫—Ä–∏—Ç–µ—Ä–∏–π –∫–∞—á–µ—Å—Ç–≤–∞ ‚Äòdeviance‚Äô (–∫—Ä–æ—Å—Å-—ç–Ω—Ç—Ä–æ–ø–∏—è) –∏–ª–∏ ‚Äòexponential‚Äô\n",
    "                                   #‚Äòdeviance‚Äô –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏ –Ω–∞ –≤—ã—Ö–æ–¥–µ\n",
    "                                   loss='deviance', \n",
    "                                   # –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —É–º–µ–Ω—å—à–µ–Ω–∏–µ –∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏—è \n",
    "                                   min_impurity_decrease=0.0, \n",
    "                                   # –£—Å—Ç–∞—Ä–µ–ª–æ\n",
    "                                   min_impurity_split=None,\n",
    "                                   # —á–∏—Å–ª–æ —É–∑–ª–æ–≤ –≤ –¥–µ—Ä–µ–≤–µ\n",
    "                                   max_depth=5,\n",
    "                                   #–º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –≤ –ø–æ—Ç–æ–º–∫–µ\n",
    "                                   min_samples_leaf=5, \n",
    "                                   #–º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –≤ —Ä–æ–¥–∏—Ç–µ–ª–µ\n",
    "                                   min_samples_split=10,\n",
    "                                   #–ü–∞—Ä–∞–º–µ—Ç—Ä, —É–º–µ–Ω—å—à–∞—é—â–∏–π –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ, —è–≤–ª—è—é—â–µ–º—Å—è –≤–µ—Å–æ–º –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –¥–µ—Ä–µ–≤–∞ (–º–µ–Ω—å—à–µ –ª—É—á—à–µ)\n",
    "                                   learning_rate=0.01                                   \n",
    "                                   )\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#–°—Ç—Ä–æ–∏–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ (–æ–±—É—á–∞—é—â–∞—è, —Ç–µ—Å—Ç–æ–≤–∞—è)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "y_pred_train = model.predict(X_train)\n",
    "print(classification_report(y_train, y_pred_train))\n",
    "\n",
    "#–°–º–æ—Ç—Ä–∏–º —Ç–æ—á–Ω–æ—Å—Ç—å(–æ—à–∏–±–∫–∏ 1 –∏ 2–≥–æ —Ä–æ–¥–∞)\n",
    "from sklearn import metrics\n",
    "conf_mat = metrics.confusion_matrix(y_test, y_pred)\n",
    "conf_mat = pd.DataFrame(conf_mat, index=model.classes_, columns=model.classes_)\n",
    "conf_mat\n",
    "\n",
    "#C–º–æ—Ç—Ä–∏–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "fi = pd.DataFrame({'features': X_train.columns, 'importance': model.feature_importances_})\n",
    "fi.sort_values('importance', ascending=False).head(10)\n",
    "\n",
    "#–ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ (–∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏)\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "model_sigmoid = CalibratedClassifierCV(model, cv=2, method='sigmoid')\n",
    "# method : ‚Äòsigmoid‚Äô or ‚Äòisotonic‚Äô\n",
    "\n",
    "# Calibrate probabilities\n",
    "model_sigmoid.fit(X_train, y_train)\n",
    "\n",
    "# View calibrated probabilities\n",
    "model_sigmoid.predict_proba(X_test)[0:11, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost - –º–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏. q - —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–µ—Ä–µ–≤–∞. \n",
    "#W - –≤–µ—Å —É–∑–ª–∞, –Ω–∞–±–æ—Ä –ø—Ä–∞–≤–∏–ª –ø–æ–ø–∞–¥–µ–Ω–∏—è –Ω–∞–±–ª—é–¥–µ–Ω–∏–π –≤ –∫–æ–Ω–µ—á–Ω—ã–π —É–∑–µ–ª. \n",
    "#T - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–Ω–µ—á–Ω—ã—Ö —É–∑–ª–æ–≤, –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è. –ê–≤—Ç–æ–º–∞—Ç–æ–º –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –≥–∞–º–º–æ–π\n",
    "#L - –∫—Ä–∏—Ç–µ—Ä–∏–π –∫–∞—á–µ—Å—Ç–≤–∞ XGBoost —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Ç—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ(Q)+—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è(1/2*–≥–∞–º–º–∞*T). –í–ª–∏—è–µ—Ç –Ω–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∫—Ä–∏—Ç–µ—Ä–∏—è —á–∏—Å—Ç–æ—Ç—ã\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "sns.set(font_scale = 1.5)\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "\n",
    "columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "           'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "           'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "df = pd.read_csv('adult.data', header=None, names=columns, na_values=' ?')\n",
    "# –£–¥–∞–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É education (–ø–æ—Ç–æ–º—É —á—Ç–æ –µ—Å—Ç—å —É–∂–µ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∫–æ–ª–æ–Ω–∫–∞ education-num)\n",
    "df = df.drop('education', axis=1)\n",
    "# –ö–æ–¥–∏—Ä—É–µ–º –æ—Ç–∫–ª–∏–∫ –≤ –±–∏–Ω–∞—Ä–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "df['income'] = df['income'].map({' <=50K': 0, ' >50K': 1})\n",
    "# —É–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NA –∑–Ω–∞—á–µ–Ω–∏—è–º–∏\n",
    "df = df.dropna()\n",
    "\n",
    "test = pd.read_csv('adult.test', header=None, names=columns, na_values=' ?', skiprows=1)\n",
    "test = test.drop('education', axis=1)\n",
    "test['income'] = test['income'].map({' <=50K.': 0, ' >50K.': 1})\n",
    "test = test.dropna()\n",
    "\n",
    "X_train = pd.get_dummies(df).drop('income', axis=1)\n",
    "y_train = df['income']\n",
    "X_test = pd.get_dummies(test).drop('income', axis=1)\n",
    "y_test = test['income']\n",
    "\n",
    "columns = set(X_train.columns) | set(X_test.columns)\n",
    "X_train = X_train.reindex(columns=columns).fillna(0)\n",
    "X_test = X_test.reindex(columns=columns).fillna(0)\n",
    "\n",
    "\n",
    "#–°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å. –í –≤—ã–±–æ—Ä–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ–º–æ–≥–∞—é—Ç grid_search –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
    "model = XGBClassifier(seed=42, \n",
    "                      booster=gbtree, #–≤—ã–±–æ—Ä –¥–µ—Ä–µ–≤—å–µ–≤ gbtree –∏–ª–∏ –ª–∏–Ω–µ–π–Ω—ã—Ö gblinear\n",
    "                      silent=True, #–ù–ï –≤—ã–≤–æ–¥ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "                      n_estimators=100, #–ü—Ä–µ–¥–µ–ª—å–Ω–æ–µ —á–∏—Å–ª–æ –¥–µ—Ä–µ–≤—å–µ–≤\n",
    "                      learning_rate=0.02, #—Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è\n",
    "                      min_child_weight=5, #–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –≤–µ—Å–∞ –≤ —É–∑–ª–µ-–ø–æ—Ç–æ–º–∫–µ\n",
    "                      max_leaf_nodes=6, #–ú–∞–∫—Å. –∑–Ω–∞—á–µ–Ω–∏–µ –∫–æ–Ω–µ—á–Ω—ã—Ö —É–∑–ª–æ–≤ –≤ –¥–µ—Ä–µ–≤–µ\n",
    "                      max_depth=5, #–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —á–∏—Å–ª–æ —Å–ª–æ–µ–≤ –¥–µ—Ä–µ–≤–∞\n",
    "                      gamma=0.05, #–ó–∞–ø—Ä–µ—â–∞–µ—Ç —Ä–∞—Å—â–µ–ø–ª–µ–Ω–∏–µ, –µ—Å–ª–∏ –∑–∞–≥—Ä—è–∑–Ω–µ–Ω–∏–µ —É–º–µ–Ω—å—à–∏–ª–æ—Å—å –º–µ–Ω—å—à–µ —á–µ–º –Ω–∞ –≥–∞–º–º—É\n",
    "                      subsample=0.7, #–î–æ–ª—è –Ω–∞–±–ª—é–¥–µ–Ω–∏–π, –ø–æ–ø–∞–¥–∞—é—â–∏—Ö –≤ —Å–ª—É—á–∞–π–Ω—É—é –ø–æ–¥–≤—ã–±–æ—Ä–∫—É\n",
    "                      reg_lambda=0, #–ö—Ä–∏—Ç–µ—Ä–∏–π –∫–∞—á–µ—Å—Ç–≤–∞, —Å—É–º–º–∞ –∫–≤–∞–¥—Ä–∞—Ç–æ–≤, mse \n",
    "                      reg_alpha=1 #–ö—Ä–∏—Ç–µ—Ä–∏–π –∫–∞—á–µ—Å—Ç–≤–∞, —Å—É–º–º–∞ –º–æ–¥—É–ª–µ–π, mae                      \n",
    "                     )\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "print (classification_report(y_train, y_pred_train))\n",
    "y_pred = model.predict(X_test)\n",
    "print (classification_report(y_test, y_pred))\n",
    "\n",
    "xgb.plot_importance(model)\n",
    "xgb.plot_importance(model, max_num_features = 30)\n",
    "            \n",
    "#–í –≤—ã–±–æ—Ä–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ–º–æ–≥–∞—é—Ç grid_search –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
    "#–û–ø—Ç–∏–º–∞–ª—å–Ω–æ –ø–µ—Ä–µ–±–∏—Ä–∞—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –ø–æ –æ–¥–Ω–æ–º—É (–Ω–æ –Ω–∞–∏–ª—É—á—à–µ–µ —Ä–µ—à–µ–Ω–∏–µ –º–æ–∂–Ω–æ –∏ –Ω–µ –Ω–∞–π—Ç–∏)\n",
    "\n",
    "smart_xgboost = GridSearchCV(cv=5, #–ü–∞—Ä–∞–º–µ—Ç—Ä —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –¥–ª—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏ (—Ñ–æ–ª–¥—ã)\n",
    "                   error_score='raise',\n",
    "                   estimator=XGBClassifier( #–ó–∞–¥–∞–µ–º –æ—Ü–µ–Ω–∫—É = XGBClassifier\n",
    "                               base_score=0.5, #–ó–∞–¥–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —Å–æ–±–∏—Ä–∞–µ–º—Å—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å\n",
    "                               colsample_bylevel=1, \n",
    "                               colsample_bytree=0.8,\n",
    "                               gamma=0, \n",
    "                               max_delta_step=0,\n",
    "                               missing=None, \n",
    "                               nthread=-1,\n",
    "                               objective='binary:logistic', #–ö—Ä–∏—Ç–µ—Ä–∏–π –∫–∞—á-–≤–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏. –∑–¥–µ—Å—å 2, –±–æ–ª—å—à–µ-softmax, sofprob\n",
    "                               num_class=3, #–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –∑–∞–¥–∞—á–µ–º —á–∏—Å–ª–æ –∫–ª–∞—Å—Å–æ–≤ –≤ –∑–∞–¥–∞—á–µ                                \n",
    "                               reg_alpha=0, \n",
    "                               reg_lambda=1,\n",
    "                               scale_pos_weight=1, \n",
    "                               seed=1234, \n",
    "                               silent=True, \n",
    "                               subsample=0.8\n",
    "                                ),\n",
    "                   fit_params={}, \n",
    "                   iid=True, \n",
    "                   n_jobs=-1,\n",
    "                   param_grid={ #–ò–∑–º–µ–Ω—è–µ–º—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
    "                               'min_child_weight': [1, 3, 5], \n",
    "                               'max_depth': [3, 5, 7]\n",
    "                               'n_estimators': [100, 300, 500, 800, 1000],\n",
    "                               'learning_rate': [0.05, 0.1, 0.3]\n",
    "                              },\n",
    "                   pre_dispatch='2*n_jobs', \n",
    "                   refit=True, \n",
    "                   scoring='accuracy', \n",
    "                   verbose=0\n",
    "                  )\n",
    "\n",
    "smart_xgboost.fit(X_train, y_train)\n",
    "\n",
    "sorted(smart_xgboost.cv_results_.keys())\n",
    "\n",
    "smart_xgboost.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#–ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤. –ù—É–∂–Ω–∞ –¥–ª—è —Ä–∞–±–æ—á–∏—Ö –º–æ–º–µ–Ω—Ç–æ–≤, —á—Ç–æ–±—ã –ª—É—á—à–µ –ø–æ–Ω—è—Ç—å –º–∞—à–∏–Ω—É \n",
    "#–£—Ç–æ—á–Ω–µ–Ω–∏–µ –≤—ã–¥–∞–Ω–Ω—ã—Ö –º–∞—à–∏–Ω–æ–π –∑–Ω–∞—á–µ–Ω–∏–π, —Å —É—á–µ—Ç–æ–º \"–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏\" –ø–æ–ø–∞—Å—Ç—å –≤ –∫–ª–∞—Å—Å\n",
    "#–ú–æ–∂–Ω–æ –∑–∞–¥–∞—Ç—å –ø–æ—Ä–æ–≥–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –æ–ø—Ä–µ–¥–µ–ª—è—Ç –∫–ª–∞—Å—Å. –í –ø—Ä–æ–º–µ–∂—É—Ç–∫–µ –º–µ–∂–¥—É –Ω–∏–º–∏ –º–∞—à–∏–Ω–∞ —Å–∫–∞–∂–µ—Ç \"–Ω–µ –∑–Ω–∞—é\"\n",
    "#–ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ - —ç—Ç–æ –ø–µ—Ä–µ—Å—á–µ—Ç –≤—ã—Ö–æ–¥–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π, —á—Ç–æ–±—ã –ø—Ä–æ –Ω–∏—Ö –º.–±. —Å–∫–∞–∑–∞—Ç—å - —ç—Ç–æ –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å\n",
    "#–ò–∑–æ—Ç–æ–Ω–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è - —ç—Ç–æ –ª–∏–Ω–∏—è, —É –∫–æ—Ç–æ—Ä–æ–π –≤–µ–∫—Ç–æ—Ä –Ω–µ —É–±—ã–≤–∞–µ—Ç\n",
    "#–õ–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –∫—Ä–∏–≤–∞—è, –º–µ—Ç–æ–¥ –ü–ª–∞—Ç—Ç–∞ - –Ω–µ—É–±—ã–≤–∞—é—â–∞—è –ª–∏–Ω–∏—è, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–∏–±–ª–∏–∑–∏—Ç Pi –∫ P\n",
    "\n",
    "\n",
    "# –°—Ç—Ä–æ–∏–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∞—Ç—å –∫ –∫–ª–∞—Å—Å—É\n",
    "y_pred_train2 = model.predict_proba(X_train)\n",
    "y_pred_test2 = model.predict_proba(X_test)\n",
    "\n",
    "#–ó–∞–≤–µ—Ä—à–∞–µ–º –ø–æ—Å—Ç—Ä–æ–µ–Ω–Ω—É—é –º–∞—à–∏–Ω—É –∫–∞–ª–∏–±—Ä–æ–≤–∫–æ–π\n",
    "model_sigmoid = CalibratedClassifierCV(model, \n",
    "                                       cv=2, \n",
    "                                       method='sigmoid' #–ò–ª–∏ \"isotonic\"\n",
    "                                      )\n",
    "\n",
    "\n",
    "#–û–±—É—á–∞–µ–º –∫–∞–ª–∏–±—Ä–æ–≤–∫—É –Ω–∞ –≤—ã–±–æ—Ä–∫–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
    "model_sigmoid.fit(X_train, y_train)\n",
    "\n",
    "#–°–º–æ—Ç—Ä–∏–º –Ω–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ø–æ—Å–ª–µ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏\n",
    "model_sigmoid.predict_proba(X_test)\n",
    "\n",
    "# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∞ –¥–ª—è –Ω–æ–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤\n",
    "new_item = [1, 1, 1, 1]\n",
    "model.predict([new_item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
