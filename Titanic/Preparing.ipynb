{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from functools import reduce\n",
    "\n",
    "#Расщепление на обучающую и тестовую выборки\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "pd.options.display.max_rows = 200\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "sns.set_style(style='white') \n",
    "sns.set(rc={\n",
    "    'figure.figsize':(12,7), \n",
    "    'axes.facecolor': 'white',\n",
    "    'axes.grid': True, 'grid.color': '.9',\n",
    "    'axes.linewidth': 1.0,\n",
    "    'grid.linestyle': u'-'},font_scale=1.5)\n",
    "custom_colors = [\"#3498db\", \"#95a5a6\",\"#34495e\", \"#2ecc71\", \"#e74c3c\"]\n",
    "sns.set_palette(custom_colors)\n",
    "\n",
    "os.chdir(r'C:\\Users\\Mr Alex\\Documents\\GitHub\\FlightPreparence')\n",
    "\n",
    "df = pd.read_csv('../Kaggle/Titanic/train.csv', sep=\"\\t\", header = 0, encoding='cp1251', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Смотрим коэффициенты корелляций. Мало больших значений - плохо для факторного анализа\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Текущая длина фрейма: ', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Удаляем строки с NaN значениями\n",
    "df = df.loc[~df['data'].isin(['NaN'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Если несколько классов, но хочется сделать классификацию строго бинарной, то разбиваем на группы ДА и НЕТ\n",
    "df['date'] = df['date'].replace(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Преобразуем объектные столбцы в численные. \n",
    "df['date'] = df['date'].map({'S': 0, 'C': 1, 'Q': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Преобразуем Dtype колонки в bool\n",
    "df['date'] = df['date'].astype('bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Преобразуем нечисловые данные в числовые\n",
    "df['data'] = LabelEncoder().fit_transform(df['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Разбиваем колонки на новые, в каждой из которых только свои уникальные значения из прежней колонки\n",
    "pd.get_dummies(df.Data, prefix=\"Emb\", drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Фильтр фрейма, оставить только строки с опредленным значением в одной из колонок\n",
    "print( 'Before:', len(df) )\n",
    "gwa_codes = [code for code in df.Code.unique() if 'GWA_' in code]\n",
    "df = df[df.Code.isin(gwa_codes)]\n",
    "print( 'After:', len(df) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Удаляем лишние колонки\n",
    "columns = ['Name', 'Ticket', 'Cabin', 'Fare']\n",
    "df.drop(columns, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удалим первые 10 строк\n",
    "df = df.drop(np.arange(10), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удалим всех пассажиров с переменной меньше 10 и больше 50 \n",
    "df1 = df.drop(df[(df['data'] < 10) | (df['data'] > 50)].index)\n",
    "df1.shape[0] / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Заменим все пропущенные значения одной из переменных из колонки\n",
    "df.Data.fillna(df.Data.mode()[0], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Категориальные колонки не имеют естественного порядка, поэтому преобразуем их с помощью one-hot encoding\n",
    "features = ['predict1', 'predict2', 'predict3']\n",
    "df = pd.get_dummies(df, columns=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Преобразуем бинарные столбцы в численные. Колонку y тоже\n",
    "df['data'] = df['data'].map({'predict1': 0, 'predict2': 1, 'predict3': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Приводим множество названий колонок к типу set, находим разность двух множеств: \n",
    "print(set(X_train.columns) - set(X_test.columns))\n",
    "print(set(X_test.columns) - set(X_train.columns))\n",
    "\n",
    "#Добавляем недостающую колонку. Смотрим, стоит ли склеивать отдельные переменные в более крупные классы\n",
    "columns = set(X_train.columns) | set(X_test.columns)\n",
    "X_train = X_train.reindex(columns=columns).fillna(0)\n",
    "X_test = X_test.reindex(columns=columns).fillna(0)\n",
    "\n",
    "#Проверяем совпадение колонок (если да, то True)\n",
    "all(X_train.columns == X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Анализ объектов в колонке. Выделение уникальных, сплит по знакам и выделение в отдельную колонку \n",
    "df['data_new'] = df.Data.apply(lambda name: name.split(',')[1].split('.')[0].strip()) \n",
    "df.Data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Группировка колонки по двум другим переменным. Медиана группы подставлена в пропущенные строки \n",
    "grp = df.groupby(['data1', 'data2'])  \n",
    "df.Data = grp.Data.apply(lambda x: x.fillna(x.median()))\n",
    "df.Data.fillna(df.Data.median, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Построение облачного графика из объектов, где размер коррелирует с частотой\n",
    "wc = WordCloud(width = 1000,height = 450,background_color = 'white').generate(str(df.Data_new.values))\n",
    "plt.imshow(wc, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()\n",
    "\n",
    "df.Data_new.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Группируем\n",
    "grp = df.groupby(['data1', 'data2'])  \n",
    "df.Data = grp.Data.apply(lambda x: x.fillna(x.median()))\n",
    "\n",
    "#If still any row remains\n",
    "td.Age.fillna(td.Age.median, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Если в БД нет единой метрики, то стандартизируем данные\n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "norm = preprocessing.StandardScaler()\n",
    "norm.fit(df)\n",
    "X = norm.transform(df)\n",
    "\n",
    "#Cтандартизируем переменные 2\n",
    "df_scaled = preprocessing.scale(df)\n",
    "\n",
    "#Методом поиска главных компонентов проецируем данные на двумерную плоскость и получаем ранжирование компонентов по важности \n",
    "pca = PCA(n_components=5).fit(df_scaled) #Уточняем число компонент и источник данных \n",
    "\n",
    "#Доля разброса в данных, объясняемая главными компонентами\n",
    "print('Влияние компонентов на общий разброс данных: ', pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Повернем фрейм, разбив один столбец на несколько, в зависимости от значений, все объекты выровнены по дате\n",
    "pivoted_df = df.pivot(index='Date', columns='Code', values='VWAP')\n",
    "pivoted_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
