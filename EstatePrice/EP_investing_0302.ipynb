{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foreword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi Kagglers!\n",
    "The score of the House Price predictions project https://www.kaggle.com/godzill22/house-price-predictions/comments I did a year ago it's nothing to be proud of. Over that period I've learnt more interesting techniques that I would like to try and see whether I can impove my model. This notebook is an attempt of systematic approach of how to deal with high dimension dataset and with different features type. I think, the data from House Price competition is ideal to practice those skills. Overall, preparation range anywhere from 60â€“80% of the total time spent on a Data Science project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:26:43.399783Z",
     "start_time": "2021-02-02T10:26:42.400789Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import missingno\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "import scipy as sp\n",
    "from scipy.stats import skew\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:26:43.440788Z",
     "start_time": "2021-02-02T10:26:43.400764Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the training dataset\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:26:43.445764Z",
     "start_time": "2021-02-02T10:26:43.442763Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:26:43.451763Z",
     "start_time": "2021-02-02T10:26:43.447763Z"
    }
   },
   "outputs": [],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:26:43.479764Z",
     "start_time": "2021-02-02T10:26:43.452763Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df[train_df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:26:43.495764Z",
     "start_time": "2021-02-02T10:26:43.480765Z"
    }
   },
   "outputs": [],
   "source": [
    "train_missing = train_df.isna().sum()\n",
    "\n",
    "train_missing = 100 * (train_missing[train_missing > 0] / len(train_df))\n",
    "train_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:26:43.530788Z",
     "start_time": "2021-02-02T10:26:43.496784Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:26:43.534785Z",
     "start_time": "2021-02-02T10:26:43.532765Z"
    }
   },
   "outputs": [],
   "source": [
    "#test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:26:43.562791Z",
     "start_time": "2021-02-02T10:26:43.536763Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df[test_df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:26:43.572763Z",
     "start_time": "2021-02-02T10:26:43.563763Z"
    }
   },
   "outputs": [],
   "source": [
    "test_missing = test_df.isna().sum()\n",
    "\n",
    "test_missing = 100 * (test_missing[test_missing > 0] / len(test_df))\n",
    "test_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check target column first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:26:43.764764Z",
     "start_time": "2021-02-02T10:26:43.573793Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "sns.distplot(train_df['SalePrice'], bins=30, kde=True, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:26:43.952764Z",
     "start_time": "2021-02-02T10:26:43.765788Z"
    }
   },
   "outputs": [],
   "source": [
    "# One way of doing it\n",
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "sns.distplot(np.log1p(train_df['SalePrice']), bins=30, kde=True, ax=ax);\n",
    "# Perform log transformation \n",
    "train_df['SalePrice'] = np.log1p(train_df['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:26:43.957765Z",
     "start_time": "2021-02-02T10:26:43.953783Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df['SalePrice'].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all I will concatenate these 2 dataframe for feature engineering. It will help me to avoid a problem where train and test dataset discrete features are different from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:26:43.973787Z",
     "start_time": "2021-02-02T10:26:43.958765Z"
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate train/test datasets\n",
    "df = pd.concat([train_df, test_df], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:26:43.982765Z",
     "start_time": "2021-02-02T10:26:43.975763Z"
    }
   },
   "outputs": [],
   "source": [
    "# Change these features into object type\n",
    "change_type = ['MSSubClass','OverallQual','OverallCond','YearBuilt','YearRemodAdd','GarageYrBlt','MoSold','YrSold']\n",
    "\n",
    "for col in change_type:\n",
    "    df[col] = df[col].astype(\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:26:44.059763Z",
     "start_time": "2021-02-02T10:26:43.983765Z"
    }
   },
   "outputs": [],
   "source": [
    "# Describe numeric columns\n",
    "df.drop(\"Id\", axis=1).describe(include=['number']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:26:44.067765Z",
     "start_time": "2021-02-02T10:26:44.060764Z"
    }
   },
   "outputs": [],
   "source": [
    "num_feat = [x for x in df.columns if df[x].dtype !=\"object\"]\n",
    "\n",
    "num_feat.remove(\"Id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:26:50.943795Z",
     "start_time": "2021-02-02T10:26:44.068764Z"
    }
   },
   "outputs": [],
   "source": [
    "# Correlation between numerical variables\n",
    "corr_matrix = df[num_feat].corr()\n",
    "plt.figure(figsize=(16,12))\n",
    "sns.heatmap(corr_matrix.T, annot=True, cbar=False, cmap='coolwarm');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:26:51.981789Z",
     "start_time": "2021-02-02T10:26:50.944763Z"
    }
   },
   "outputs": [],
   "source": [
    "# Correlated variables greater than 0.8\n",
    "corr_matrix = df[num_feat].corr()\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.heatmap(corr_matrix.T, annot=True, mask= corr_matrix < 0.8 ,cbar=False, cmap='coolwarm');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check how these correlated variables to each other are correlated to the target column, so I can decide which of them remove from further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:26:52.302763Z",
     "start_time": "2021-02-02T10:26:51.982800Z"
    }
   },
   "outputs": [],
   "source": [
    "price_corr_ser = df[num_feat].corr()['SalePrice']\n",
    "price_corr_ser = price_corr_ser.sort_values(ascending=False)\n",
    "price_corr_ser = price_corr_ser.drop(\"SalePrice\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,12))\n",
    "sns.barplot(x=price_corr_ser.values, y=price_corr_ser.index, palette=\"rocket_r\")\n",
    "plt.title(\"Numeric Feature Correlation with Traget Column\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:26:52.309763Z",
     "start_time": "2021-02-02T10:26:52.303763Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove one of the highly correlated variables\n",
    "high_correlated_var = [\"GarageArea\",'1stFlrSF','TotRmsAbvGrd']\n",
    "df = df.drop(high_correlated_var, axis=1)\n",
    "\n",
    "# Remove it from list of numeric columns\n",
    "for c in high_correlated_var:\n",
    "    num_feat.remove(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:27:00.627278Z",
     "start_time": "2021-02-02T10:26:52.310789Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot distribution of numeric variables\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "\n",
    "for i in range(len(num_feat)):\n",
    "    plt.subplot(14,5, i+1)\n",
    "    sns.distplot(df[num_feat[i]], rug=True, hist=False, kde_kws={'bw':0.1})\n",
    "    plt.title(num_feat[i])\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:27:07.228790Z",
     "start_time": "2021-02-02T10:27:00.628300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize relation between numeric features and target column\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "# numeric_df = num_df.drop('SalePrice', axis=1)\n",
    "\n",
    "for i, col in enumerate(df[num_feat].columns):\n",
    "    plt.subplot(12,5, i+1)\n",
    "    sns.scatterplot(x=df[col], y=df['SalePrice'])\n",
    "    plt.tight_layout()\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:27:07.688790Z",
     "start_time": "2021-02-02T10:27:07.229788Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(24,15))\n",
    "\n",
    "plt.subplot(4,3,1)\n",
    "sns.distplot(df[\"LotArea\"])\n",
    "\n",
    "plt.subplot(4,3,2)\n",
    "sns.scatterplot(x=\"LotArea\", y=\"SalePrice\", data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:27:07.698789Z",
     "start_time": "2021-02-02T10:27:07.691788Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"LotArea\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will remove outliers from this continues numeric column later on as it would effect my test dataset for submission if I do it now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PoolArea**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:27:07.712788Z",
     "start_time": "2021-02-02T10:27:07.700789Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create binary column 1 if the house has a pool, 0 if not\n",
    "df['isPool'] = df['PoolArea'].apply(lambda x: 0 if x == 0 else 1)\n",
    "df['isPool'] = df['isPool'].astype(\"object\")\n",
    "df = df.drop('PoolArea',axis=1)\n",
    "num_feat.remove(\"PoolArea\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**totalPorch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:27:07.729788Z",
     "start_time": "2021-02-02T10:27:07.713789Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a new column where I concatenate all Porch columns\n",
    "porch_col = ['OpenPorchSF','EnclosedPorch', '3SsnPorch', 'ScreenPorch']\n",
    "\n",
    "df['totalPorch'] = np.zeros(len(df)).reshape(len(df),1)\n",
    "\n",
    "for col in porch_col:\n",
    "    df['totalPorch'] += df[col]\n",
    "    \n",
    "# Remove porch col from dataset\n",
    "for c in porch_col:\n",
    "    df.drop(c, axis=1, inplace=True)\n",
    "\n",
    "# Remove it from the list of numerical columns\n",
    "to_remove = [\"OpenPorchSF\", \"EnclosedPorch\", \"3SsnPorch\", \"ScreenPorch\"]\n",
    "for c in to_remove:\n",
    "    num_feat.remove(c)\n",
    "\n",
    "# Add column to the list of numeric\n",
    "num_feat.append(\"totalPorch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bathroom columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:27:07.747788Z",
     "start_time": "2021-02-02T10:27:07.730788Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create new columns and drop relevant ones\n",
    "df[\"TotBathAbvGrade\"] = df[\"FullBath\"] + (0.5 * df[\"HalfBath\"])\n",
    "df[\"TotBsmtBath\"] = df[\"BsmtFullBath\"] + (0.5 * df[\"BsmtHalfBath\"])\n",
    "\n",
    "# Remove columns\n",
    "to_remove = [\"FullBath\",\"HalfBath\",\"BsmtFullBath\", \"BsmtHalfBath\"]\n",
    "\n",
    "for col in to_remove:\n",
    "    df.drop(col, axis=1, inplace=True)\n",
    "    num_feat.remove(col)\n",
    "\n",
    "# Append new ones to the numeric columns\n",
    "num_feat.append(\"TotBathAbvGrade\")\n",
    "num_feat.append(\"TotBsmtBath\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns LotFrontage(Linear feet of street connected to property) and LotArea(Lot size in square feet) are highly correlated, so I will drop feature with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:27:07.754789Z",
     "start_time": "2021-02-02T10:27:07.748789Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove useless numerical column\n",
    "df.drop(\"LotFrontage\", axis=1, inplace=True)\n",
    "num_feat.remove(\"LotFrontage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:27:11.411134Z",
     "start_time": "2021-02-02T10:27:07.755789Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a plot again\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "\n",
    "for i, col in enumerate(num_feat):\n",
    "    plt.subplot(12,5, i+1)\n",
    "    sns.scatterplot(x=df[col], y='SalePrice', data=df)\n",
    "    plt.tight_layout()\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values in numeric features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only numerical column left with some missing values (less than 1%) so I will fill them with a mean of the column. I don't need to fill missing values in SalePrice Columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:27:11.711139Z",
     "start_time": "2021-02-02T10:27:11.412112Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show missing values\n",
    "missingno.matrix(df[num_feat], figsize=(20,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:27:11.716110Z",
     "start_time": "2021-02-02T10:27:11.713110Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove SalePrice temporary\n",
    "num_feat.remove(\"SalePrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:27:12.364112Z",
     "start_time": "2021-02-02T10:27:11.717110Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "imp.fit(df[num_feat])\n",
    "df[num_feat] = imp.transform(df[num_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:27:12.423112Z",
     "start_time": "2021-02-02T10:27:12.365112Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in df[num_feat]:\n",
    "    df[col] = df[col].apply(lambda x: np.log1p(x))\n",
    "    \n",
    "# Append SalePrice back to numeric columns\n",
    "num_feat.append(\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Values in Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:28:20.401608Z",
     "start_time": "2021-02-02T10:27:12.424138Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of categorical columns\n",
    "cat_feat = [x for x in df.columns if df[x].dtype == \"object\"]\n",
    "\n",
    "# Create a multi plot with categorical features\n",
    "fig = plt.figure(figsize=(18, 30))\n",
    "\n",
    "for i , col in enumerate(cat_feat):\n",
    "    plt.subplot(12,5, i+1)\n",
    "    sns.boxplot(x=col, y='SalePrice', data=df)\n",
    "    plt.ylabel(\"Log() SalePrice\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some features that are useless(to many variables or the same information). First, I will try to create new features from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:28:20.686578Z",
     "start_time": "2021-02-02T10:28:20.407578Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_missing = df[cat_feat].isna().sum()\n",
    "\n",
    "cat_missing = 100 * (cat_missing[cat_missing > 0] / len(df[cat_feat]))\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x= cat_missing.sort_values(ascending=False).values, y= cat_missing.sort_values(ascending=False).index)\n",
    "plt.title(\"Missing Categorical Values in %\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing data in categorical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many methods to impute data, some of them are very sophisticated, but there is one flaw, we impute artificially created values. In case of categorical variables imputing mode of a column could be one of them, but I will fill missing values with a string \"None\" so I could retain the orginal information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:28:20.714588Z",
     "start_time": "2021-02-02T10:28:20.687577Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fill missing values in categorical columns with a string\n",
    "for col in cat_feat:\n",
    "    if df[col].isna().sum() > 0:\n",
    "        df[col] = df[col].fillna(value=\"NA\")\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:28:20.903579Z",
     "start_time": "2021-02-02T10:28:20.715577Z"
    }
   },
   "outputs": [],
   "source": [
    "missingno.matrix(df[cat_feat], figsize=(20,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:28:20.907579Z",
     "start_time": "2021-02-02T10:28:20.904578Z"
    }
   },
   "outputs": [],
   "source": [
    "# for col in cat_missing.columns:\n",
    "    # print(f\" Column '{col}' has unique values {df[col].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:28:20.923580Z",
     "start_time": "2021-02-02T10:28:20.908581Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_missing = df[cat_feat].isna().sum()\n",
    "\n",
    "cat_missing = 100 * (cat_missing[cat_missing > 0] / len(df[cat_feat]))\n",
    "\n",
    "cat_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering for categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GarageYrBlt column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:28:21.112602Z",
     "start_time": "2021-02-02T10:28:20.924578Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating new series \n",
    "is_garage = df['GarageYrBlt'].apply(lambda x: 1 if x != \"NA\" else 0)\n",
    "\n",
    "# Plot new series\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,4))\n",
    "\n",
    "sns.countplot(is_garage, ax=axes[0])\n",
    "sns.boxplot(x=is_garage.values, y='SalePrice', data=df, ax=axes[1])\n",
    "\n",
    "axes[0].set_xlabel(\"Is Garage\")\n",
    "axes[0].set_ylabel(\"SalePrice \")\n",
    "\n",
    "axes[1].set_xlabel(\"Is Garage\")\n",
    "axes[1].set_ylabel(\"SalePrice \");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:28:21.117579Z",
     "start_time": "2021-02-02T10:28:21.113579Z"
    }
   },
   "outputs": [],
   "source": [
    "to_remove = []\n",
    "# Add to the list of columns to remove\n",
    "to_remove.append(\"GarageYrBlt\")\n",
    "\n",
    "# Create new column from GaragYrBlt\n",
    "df['isGarage'] = is_garage.astype('object')\n",
    "cat_feat.append(\"isGarage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YearRemodAdd column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:28:21.123578Z",
     "start_time": "2021-02-02T10:28:21.118578Z"
    }
   },
   "outputs": [],
   "source": [
    "df['YearRemodAdd'].unique()\n",
    "to_remove.append(\"YearRemodAdd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't see any value from this column, therefore I will drop it later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YearBuilt & YrSold columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:28:21.130598Z",
     "start_time": "2021-02-02T10:28:21.124579Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a series of how old a house was when sold\n",
    "how_old = (df['YrSold'].astype(int) - df['YearBuilt'].astype(int))\n",
    "\n",
    "# New column from \n",
    "df['Old_in_Years'] = pd.Series(how_old)\n",
    "# Update to numertic list\n",
    "num_feat.append(\"Old_in_Years\")\n",
    "\n",
    "# Add columns for remove\n",
    "to_remove.append('YrSold')\n",
    "to_remove.append('YearBuilt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:28:21.402606Z",
     "start_time": "2021-02-02T10:28:21.131578Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,10))\n",
    "\n",
    "# Distribution of new column\n",
    "plt.subplot(4,2, 1)\n",
    "sns.distplot(df['Old_in_Years'])\n",
    "plt.ylabel(\"count\")\n",
    "\n",
    "# Scatterplot of new column\n",
    "plt.subplot(4,2, 2)\n",
    "sns.scatterplot(x=df['Old_in_Years'].values, y='SalePrice', data=df)\n",
    "\n",
    "# Labels\n",
    "plt.xlabel(\"Old in Years\")\n",
    "plt.ylabel(\"SalePrice\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not sure if creating another column from two old ones will improve my model or it will carry the same information as newly created numerical one? If someone can clear that for me that would be great. For now I won't create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:28:21.408579Z",
     "start_time": "2021-02-02T10:28:21.403579Z"
    }
   },
   "outputs": [],
   "source": [
    "to_remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Condition1 & Condition2 columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:28:21.415579Z",
     "start_time": "2021-02-02T10:28:21.409578Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Condition1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:28:21.422598Z",
     "start_time": "2021-02-02T10:28:21.416579Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Condition2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:28:21.622579Z",
     "start_time": "2021-02-02T10:28:21.423579Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,10))\n",
    "\n",
    "condition1 = df['Condition1'].apply(lambda x: x if x == \"Norm\" else \"Other\")\n",
    "\n",
    "plt.subplot(3,2, 1)\n",
    "sns.countplot(condition1)\n",
    "\n",
    "plt.subplot(3,2, 2)\n",
    "sns.boxplot(x=condition1.values, y='SalePrice', data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:28:21.788579Z",
     "start_time": "2021-02-02T10:28:21.623580Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,10))\n",
    "\n",
    "condition2 = df['Condition2'].apply(lambda x: x if x == \"Norm\" else \"Other\")\n",
    "\n",
    "plt.subplot(4,2, 1)\n",
    "sns.countplot(condition2)\n",
    "\n",
    "plt.subplot(4,2,2)\n",
    "sns.boxplot(x=condition2.values, y='SalePrice', data=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think, that the only reason doing it is to reduce dimensionality of our dataframe. I am going to to keep these columns in unchange form now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nominal and Ordinal Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the categorical columns have nominal or ordinal values and that needs to be addressed. As a remminder, nominal data is when we can only classify the data, while ordinal data can be classified and ordered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ordinal values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:28:21.800580Z",
     "start_time": "2021-02-02T10:28:21.789577Z"
    }
   },
   "outputs": [],
   "source": [
    "ordinal_feat = ['OverallQual','OverallCond','ExterQual','ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure','BsmtFinType1',\n",
    "               'BsmtFinType2','HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual','GarageCond','PoolQC']\n",
    "\n",
    "df['BsmtExposure'] = df['BsmtExposure'].apply(lambda x: x if x !='No' else \"NA\")\n",
    "\n",
    "for col in ordinal_feat:\n",
    "    # Remove ordinal from list  \n",
    "    cat_feat.remove(col)\n",
    "        \n",
    "    print(f\" Column '{col}' has unique values {df[col].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:28:21.843597Z",
     "start_time": "2021-02-02T10:28:21.802577Z"
    }
   },
   "outputs": [],
   "source": [
    "# Map ordinal columns and change their type\n",
    "ord_map = {\"NA\":0, \"Po\":1, \"Fa\":2, \"TA\":3, \"Gd\":4,\"Ex\":5}\n",
    "ord_map1 = {\"NA\":0, \"Unf\":1, \"LwQ\":2, \"Rec\":3, \"BLQ\":4, \"ALQ\":5, \"GLQ\":6}\n",
    "ord_map2 = {\"NA\":0, \"No\":1, \"Mn\":2, \"Av\":3, \"Gd\":4}\n",
    "\n",
    "for col in ordinal_feat:\n",
    "        \n",
    "    if len(df[col].unique()) <= 6 and col !=\"BsmtExposure\":\n",
    "        df[col] = df[col].map(ord_map)\n",
    "        df[col] = df[col].astype(int)\n",
    "        \n",
    "    elif col in ['OverallQual', 'OverallCond']:\n",
    "        df[col] = df[col].astype(int)\n",
    "        \n",
    "    elif df[col].name in ['BsmtFinType1', 'BsmtFinType2']:\n",
    "        df[col] = df[col].map(ord_map1)\n",
    "        df[col] = df[col].astype(int)\n",
    "        \n",
    "    else:\n",
    "        df[col] = df[col].map(ord_map2) \n",
    "        df[col] = df[col].astype(int)\n",
    "        \n",
    "for col in ordinal_feat:\n",
    "    # Append nominal features to the numerical features\n",
    "    num_feat.append(col)\n",
    "    print(f\" Column '{col}' has unique values :{df[col].unique()}, dtype: {df[col].dtypes}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:28:21.856606Z",
     "start_time": "2021-02-02T10:28:21.844577Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's remove some categorical columns we do not need anymore\n",
    "for col in to_remove:\n",
    "    df.drop(col, axis=1, inplace=True)\n",
    "    cat_feat.remove(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Check numerical features correlation again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:28:23.157604Z",
     "start_time": "2021-02-02T10:28:21.857578Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,12))\n",
    "\n",
    "sns.heatmap(df[num_feat].corr(), mask= df[num_feat].corr()  < 0.8 , cbar=False, cmap='coolwarm', annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I introduce correlation between features when I converted some ordinal features and I need to remove one of the correlated feature. There is also high correlation between \"SalePrice\" and \"OverallQual\" (0.817185) but this is all right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:28:23.169580Z",
     "start_time": "2021-02-02T10:28:23.158580Z"
    }
   },
   "outputs": [],
   "source": [
    "corr_to_remove = ['GarageCond', 'FireplaceQu', 'BsmtFinType1','BsmtFinType2','BsmtCond',]\n",
    "\n",
    "for col in corr_to_remove:\n",
    "    df = df.drop(col, axis=1)\n",
    "    num_feat.remove(col)\n",
    "    ordinal_feat.remove(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nominal values**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different methods can be apply to convert nominal variables into numbers so I future algorithm can work with them. All of them have prons and cons but I am not going to write about it here. One of the simpliest and easy to understand is pandas \"get_dummies\" method, however you need to remember not to indroduce nulticollinearity what is also called (dummy trap). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:28:23.235670Z",
     "start_time": "2021-02-02T10:28:23.170578Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create dummy variables \n",
    "dummy_df = pd.get_dummies(df[cat_feat], drop_first=True)\n",
    "\n",
    "for col in cat_feat:\n",
    "    df.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "df_with_dummies = pd.concat([df, dummy_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second method for converting nominal categorical variables is OneHotEncode, but I am not going to use it in this notebook. Another common method used by practitioners is Label Encoding which suits more with variables who have some sort of order. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, this is very important that we split dataset back into test and train dataset before we scale the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:28:23.245604Z",
     "start_time": "2021-02-02T10:28:23.236577Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split dataframe into test/train dataset\n",
    "clean_train_df = df_with_dummies[df_with_dummies[\"SalePrice\"] > 0].copy()\n",
    "clean_test_df = df_with_dummies[df_with_dummies[\"SalePrice\"].isna()].copy()\n",
    "\n",
    "# Drop SalePrice column from test set\n",
    "clean_test_df.drop(\"SalePrice\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:28:23.250581Z",
     "start_time": "2021-02-02T10:28:23.246579Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_test_df.shape, clean_train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Skewness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:28:23.263578Z",
     "start_time": "2021-02-02T10:28:23.251579Z"
    }
   },
   "outputs": [],
   "source": [
    "skewed_features = clean_train_df[num_feat].skew().sort_values(ascending=False)\n",
    "skewed_features = skewed_features[skewed_features > 0.5]\n",
    "skewed_index = skewed_features.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:28:23.492581Z",
     "start_time": "2021-02-02T10:28:23.264579Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,4), dpi=120)\n",
    "skewed_features.sort_values(ascending=False).plot(kind='bar')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xticks(horizontalalignment=\"right\")\n",
    "plt.title(\"Skewed Feature above 0.5 upper limit\")\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all I will remove highly skewed features from dataset and then trim off the rest of them by 0.5 limit. Acceptable values for skewness are between -0.5 and 0.5 while -2 and 2 for Kurtosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:28:23.501578Z",
     "start_time": "2021-02-02T10:28:23.493580Z"
    }
   },
   "outputs": [],
   "source": [
    "right_skewness_col = ['PoolQC', 'LowQualFinSF']\n",
    "for col in right_skewness_col:\n",
    "    clean_train_df.drop(col, axis=1, inplace=True)\n",
    "    clean_test_df.drop(col, axis=1, inplace= True)\n",
    "    num_feat.remove(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:28:23.505581Z",
     "start_time": "2021-02-02T10:28:23.502579Z"
    }
   },
   "outputs": [],
   "source": [
    "skewed_index = skewed_index.drop(['PoolQC','LowQualFinSF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:28:23.531678Z",
     "start_time": "2021-02-02T10:28:23.506581Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in skewed_index:\n",
    "    q3 = np.quantile(clean_train_df[col], 0.75)\n",
    "    q1 = np.quantile(clean_train_df[col], 0.25)\n",
    "    iqr = q3 - q1\n",
    "    # Upper limit for outliers\n",
    "    upper_limit = q3 + (1.5*iqr)\n",
    "    col_limit =  clean_train_df[col].apply(lambda x: x <= upper_limit)\n",
    "    clean_train_df = clean_train_df[col_limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:28:23.536579Z",
     "start_time": "2021-02-02T10:28:23.532578Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:28:23.546577Z",
     "start_time": "2021-02-02T10:28:23.537581Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save id column for submission\n",
    "row_id = pd.Series(clean_test_df[\"Id\"])\n",
    "clean_train_df = clean_train_df.drop(\"Id\", axis=1).astype(\"float64\")\n",
    "clean_test_df = clean_test_df.drop(\"Id\", axis=1).astype(\"float64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting and standarization of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:28:23.558579Z",
     "start_time": "2021-02-02T10:28:23.547604Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split training dataset into X/y first\n",
    "X = clean_train_df.drop([\"SalePrice\"], axis=1)\n",
    "y = clean_train_df['SalePrice']\n",
    "\n",
    "# Then, split it into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We all know how important is to standarize/normalize dataset for our algorithms but there are exceptions depend on what algorithm we are going to use. In order to do it right we need to remember about few things. Some suggest that it only descrete variables should be standarized and definitely we have to fit and transform train set and then only transforming test set. The reason for that we are not creating what is known as data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:28:23.562579Z",
     "start_time": "2021-02-02T10:28:23.559579Z"
    }
   },
   "outputs": [],
   "source": [
    "num_feat.remove('SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:28:23.575578Z",
     "start_time": "2021-02-02T10:28:23.563578Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaled_Xtrain = X_train.copy()\n",
    "scaled_Xtest = X_test.copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_Xtrain[num_feat] = scaler.fit_transform(scaled_Xtrain[num_feat])\n",
    "scaled_Xtest[num_feat] = scaler.transform(scaled_Xtest[num_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-02T10:28:23.593602Z",
     "start_time": "2021-02-02T10:28:23.576578Z"
    }
   },
   "outputs": [],
   "source": [
    "scaled_Xtrain.head()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "644px",
    "left": "14px",
    "right": "20px",
    "top": "228px",
    "width": "357px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
