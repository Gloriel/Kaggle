{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Foreword"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Hi Kagglers!\nThe score of the House Price predictions project https://www.kaggle.com/godzill22/house-price-predictions/comments I did a year ago it's nothing to be proud of. Over that period I've learnt more interesting techniques that I would like to try and see whether I can impove my model. This notebook is an attempt of systematic approach of how to deal with high dimension dataset and with different features type. I think, the data from House Price competition is ideal to practice those skills. Overall, preparation range anywhere from 60â€“80% of the total time spent on a Data Science project."},{"metadata":{},"cell_type":"markdown","source":"## Load the dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport missingno\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.express as px\n\nimport scipy as sp\nfrom scipy.stats import skew\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load data desctiption\n# with open(\"/kaggle/input/home-data-for-ml-course/data_description.txt\", 'r') as f:\n    # print(f.read())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the training dataset\ntrain_df = pd.read_csv(\"/kaggle/input/home-data-for-ml-course/train.csv\")\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[train_df.duplicated()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_missing = train_df.isna().sum()\n\ntrain_missing = 100 * (train_missing[train_missing > 0] / len(train_df))\ntrain_missing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the test dataset\ntest_df = pd.read_csv(\"/kaggle/input/home-data-for-ml-course/test.csv\")\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df[test_df.duplicated()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_missing = test_df.isna().sum()\n\ntest_missing = 100 * (test_missing[test_missing > 0] / len(test_df))\ntest_missing","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check target column first"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,4))\nsns.distplot(train_df['SalePrice'], bins=30, kde=True, ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# One way of doing it\nfig, ax = plt.subplots(figsize=(10,4))\nsns.distplot(np.log1p(train_df['SalePrice']), bins=30, kde=True, ax=ax);\n# Perform log transformation \ntrain_df['SalePrice'] = np.log1p(train_df['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['SalePrice'].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First of all I will concatenate these 2 dataframe for feature engineering. It will help me to avoid a problem where train and test dataset discrete features are different from each other."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Concatenate train/test datasets\ndf = pd.concat([train_df, test_df], axis=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Numerical Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Change these features into object type\nchange_type = ['MSSubClass','OverallQual','OverallCond','YearBuilt','YearRemodAdd','GarageYrBlt','MoSold','YrSold']\n\nfor col in change_type:\n    df[col] = df[col].astype(\"object\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Describe numeric columns\ndf.drop(\"Id\", axis=1).describe(include=['number']).T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_feat = [x for x in df.columns if df[x].dtype !=\"object\"]\n\nnum_feat.remove(\"Id\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlation between numerical variables\ncorr_matrix = df[num_feat].corr()\nplt.figure(figsize=(16,12))\nsns.heatmap(corr_matrix.T, annot=True, cbar=False, cmap='coolwarm');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Correlated variables greater than 0.8\ncorr_matrix = df[num_feat].corr()\nplt.figure(figsize=(12,12))\nsns.heatmap(corr_matrix.T, annot=True, mask= corr_matrix < 0.8 ,cbar=False, cmap='coolwarm');","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check how these correlated variables to each other are correlated to the target column, so I can decide which of them remove from further analysis."},{"metadata":{"trusted":true},"cell_type":"code","source":"price_corr_ser = df[num_feat].corr()['SalePrice']\nprice_corr_ser = price_corr_ser.sort_values(ascending=False)\nprice_corr_ser = price_corr_ser.drop(\"SalePrice\")\n\nfig, ax = plt.subplots(figsize=(10,12))\nsns.barplot(x=price_corr_ser.values, y=price_corr_ser.index, palette=\"rocket_r\")\nplt.title(\"Numeric Feature Correlation with Traget Column\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove one of the highly correlated variables\nhigh_correlated_var = [\"GarageArea\",'1stFlrSF','TotRmsAbvGrd']\ndf = df.drop(high_correlated_var, axis=1)\n\n# Remove it from list of numeric columns\nfor c in high_correlated_var:\n    num_feat.remove(c)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Distribution of numeric features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot distribution of numeric variables\nfig = plt.figure(figsize=(20,20))\n\nfor i in range(len(num_feat)):\n    plt.subplot(14,5, i+1)\n    sns.distplot(df[num_feat[i]], rug=True, hist=False, kde_kws={'bw':0.1})\n    plt.title(num_feat[i])\n    plt.xlabel(\"Value\")\n    plt.ylabel(\"Count\")\n    plt.tight_layout()\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize relation between numeric features and target column\nfig = plt.figure(figsize=(20,20))\n# numeric_df = num_df.drop('SalePrice', axis=1)\n\nfor i, col in enumerate(df[num_feat].columns):\n    plt.subplot(12,5, i+1)\n    sns.scatterplot(x=df[col], y=df['SalePrice'])\n    plt.tight_layout()\n    \nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Numerical outliers"},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(24,15))\n\nplt.subplot(4,3,1)\nsns.distplot(df[\"LotArea\"])\n\nplt.subplot(4,3,2)\nsns.scatterplot(x=\"LotArea\", y=\"SalePrice\", data=df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df[\"LotArea\"].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I will remove outliers from this continues numeric column later on as it would effect my test dataset for submission if I do it now."},{"metadata":{},"cell_type":"markdown","source":"**PoolArea**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create binary column 1 if the house has a pool, 0 if not\ndf['isPool'] = df['PoolArea'].apply(lambda x: 0 if x == 0 else 1)\ndf['isPool'] = df['isPool'].astype(\"object\")\ndf = df.drop('PoolArea',axis=1)\nnum_feat.remove(\"PoolArea\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**totalPorch**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a new column where I concatenate all Porch columns\nporch_col = ['OpenPorchSF','EnclosedPorch', '3SsnPorch', 'ScreenPorch']\n\ndf['totalPorch'] = np.zeros(len(df)).reshape(len(df),1)\n\nfor col in porch_col:\n    df['totalPorch'] += df[col]\n    \n# Remove porch col from dataset\nfor c in porch_col:\n    df.drop(c, axis=1, inplace=True)\n\n# Remove it from the list of numerical columns\nto_remove = [\"OpenPorchSF\", \"EnclosedPorch\", \"3SsnPorch\", \"ScreenPorch\"]\nfor c in to_remove:\n    num_feat.remove(c)\n\n# Add column to the list of numeric\nnum_feat.append(\"totalPorch\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Bathroom columns**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create new columns and drop relevant ones\ndf[\"TotBathAbvGrade\"] = df[\"FullBath\"] + (0.5 * df[\"HalfBath\"])\ndf[\"TotBsmtBath\"] = df[\"BsmtFullBath\"] + (0.5 * df[\"BsmtHalfBath\"])\n\n# Remove columns\nto_remove = [\"FullBath\",\"HalfBath\",\"BsmtFullBath\", \"BsmtHalfBath\"]\n\nfor col in to_remove:\n    df.drop(col, axis=1, inplace=True)\n    num_feat.remove(col)\n\n# Append new ones to the numeric columns\nnum_feat.append(\"TotBathAbvGrade\")\nnum_feat.append(\"TotBsmtBath\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Columns LotFrontage(Linear feet of street connected to property) and LotArea(Lot size in square feet) are highly correlated, so I will drop feature with missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove useless numerical column\ndf.drop(\"LotFrontage\", axis=1, inplace=True)\nnum_feat.remove(\"LotFrontage\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a plot again\nfig = plt.figure(figsize=(15,15))\n\nfor i, col in enumerate(num_feat):\n    plt.subplot(12,5, i+1)\n    sns.scatterplot(x=df[col], y='SalePrice', data=df)\n    plt.tight_layout()\n\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Missing values in numeric features"},{"metadata":{},"cell_type":"markdown","source":"The only numerical column left with some missing values (less than 1%) so I will fill them with a mean of the column. I don't need to fill missing values in SalePrice Columns."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Show missing values\nmissingno.matrix(df[num_feat], figsize=(20,4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove SalePrice temporary\nnum_feat.remove(\"SalePrice\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\n\nimp = SimpleImputer(missing_values=np.nan, strategy='median')\nimp.fit(df[num_feat])\ndf[num_feat] = imp.transform(df[num_feat])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in df[num_feat]:\n    df[col] = df[col].apply(lambda x: np.log1p(x))\n    \n# Append SalePrice back to numeric columns\nnum_feat.append(\"SalePrice\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Categorical features"},{"metadata":{},"cell_type":"markdown","source":"#### Missing Values in Categorical"},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of categorical columns\ncat_feat = [x for x in df.columns if df[x].dtype == \"object\"]\n\n# Create a multi plot with categorical features\nfig = plt.figure(figsize=(18, 30))\n\nfor i , col in enumerate(cat_feat):\n    plt.subplot(12,5, i+1)\n    sns.boxplot(x=col, y='SalePrice', data=df)\n    plt.ylabel(\"Log() SalePrice\")\n    plt.tight_layout()\n    \nfig.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are some features that are useless(to many variables or the same information). First, I will try to create new features from them."},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_missing = df[cat_feat].isna().sum()\n\ncat_missing = 100 * (cat_missing[cat_missing > 0] / len(df[cat_feat]))\n\nplt.figure(figsize=(10,5))\nsns.barplot(x= cat_missing.sort_values(ascending=False).values, y= cat_missing.sort_values(ascending=False).index)\nplt.title(\"Missing Categorical Values in %\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Missing data in categorical columns"},{"metadata":{},"cell_type":"markdown","source":"There are many methods to impute data, some of them are very sophisticated, but there is one flaw, we impute artificially created values. In case of categorical variables imputing mode of a column could be one of them, but I will fill missing values with a string \"None\" so I could retain the orginal information."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fill missing values in categorical columns with a string\nfor col in cat_feat:\n    if df[col].isna().sum() > 0:\n        df[col] = df[col].fillna(value=\"NA\")\n    else:\n        continue","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missingno.matrix(df[cat_feat], figsize=(20,4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for col in cat_missing.columns:\n    # print(f\" Column '{col}' has unique values {df[col].unique()}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_missing = df[cat_feat].isna().sum()\n\ncat_missing = 100 * (cat_missing[cat_missing > 0] / len(df[cat_feat]))\n\ncat_missing","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Feature engineering for categorical variables"},{"metadata":{},"cell_type":"markdown","source":"**GarageYrBlt column**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating new series \nis_garage = df['GarageYrBlt'].apply(lambda x: 1 if x != \"NA\" else 0)\n\n# Plot new series\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,4))\n\nsns.countplot(is_garage, ax=axes[0])\nsns.boxplot(x=is_garage.values, y='SalePrice', data=df, ax=axes[1])\n\naxes[0].set_xlabel(\"Is Garage\")\naxes[0].set_ylabel(\"SalePrice \")\n\naxes[1].set_xlabel(\"Is Garage\")\naxes[1].set_ylabel(\"SalePrice \");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"to_remove = []\n# Add to the list of columns to remove\nto_remove.append(\"GarageYrBlt\")\n\n# Create new column from GaragYrBlt\ndf['isGarage'] = is_garage.astype('object')\ncat_feat.append(\"isGarage\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**YearRemodAdd column**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['YearRemodAdd'].unique()\nto_remove.append(\"YearRemodAdd\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I don't see any value from this column, therefore I will drop it later on."},{"metadata":{},"cell_type":"markdown","source":"**YearBuilt & YrSold columns**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a series of how old a house was when sold\nhow_old = (df['YrSold'].astype(int) - df['YearBuilt'].astype(int))\n\n# New column from \ndf['Old_in_Years'] = pd.Series(how_old)\n# Update to numertic list\nnum_feat.append(\"Old_in_Years\")\n\n# Add columns for remove\nto_remove.append('YrSold')\nto_remove.append('YearBuilt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(18,10))\n\n# Distribution of new column\nplt.subplot(4,2, 1)\nsns.distplot(df['Old_in_Years'])\nplt.ylabel(\"count\")\n\n# Scatterplot of new column\nplt.subplot(4,2, 2)\nsns.scatterplot(x=df['Old_in_Years'].values, y='SalePrice', data=df)\n\n# Labels\nplt.xlabel(\"Old in Years\")\nplt.ylabel(\"SalePrice\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I'm not sure if creating another column from two old ones will improve my model or it will carry the same information as newly created numerical one? If someone can clear that for me that would be great. For now I won't create it."},{"metadata":{"trusted":true},"cell_type":"code","source":"to_remove","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Condition1 & Condition2 columns**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Condition1'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Condition2'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(18,10))\n\ncondition1 = df['Condition1'].apply(lambda x: x if x == \"Norm\" else \"Other\")\n\nplt.subplot(3,2, 1)\nsns.countplot(condition1)\n\nplt.subplot(3,2, 2)\nsns.boxplot(x=condition1.values, y='SalePrice', data=df);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(18,10))\n\ncondition2 = df['Condition2'].apply(lambda x: x if x == \"Norm\" else \"Other\")\n\nplt.subplot(4,2, 1)\nsns.countplot(condition2)\n\nplt.subplot(4,2,2)\nsns.boxplot(x=condition2.values, y='SalePrice', data=df);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I think, that the only reason doing it is to reduce dimensionality of our dataframe. I am going to to keep these columns in unchange form now."},{"metadata":{},"cell_type":"markdown","source":"### Nominal and Ordinal Columns"},{"metadata":{},"cell_type":"markdown","source":"Some of the categorical columns have nominal or ordinal values and that needs to be addressed. As a remminder, nominal data is when we can only classify the data, while ordinal data can be classified and ordered."},{"metadata":{},"cell_type":"markdown","source":"**Ordinal values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"ordinal_feat = ['OverallQual','OverallCond','ExterQual','ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure','BsmtFinType1',\n               'BsmtFinType2','HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual','GarageCond','PoolQC']\n\ndf['BsmtExposure'] = df['BsmtExposure'].apply(lambda x: x if x !='No' else \"NA\")\n\nfor col in ordinal_feat:\n    # Remove ordinal from list  \n    cat_feat.remove(col)\n        \n    print(f\" Column '{col}' has unique values {df[col].unique()}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Map ordinal columns and change their type\nord_map = {\"NA\":0, \"Po\":1, \"Fa\":2, \"TA\":3, \"Gd\":4,\"Ex\":5}\nord_map1 = {\"NA\":0, \"Unf\":1, \"LwQ\":2, \"Rec\":3, \"BLQ\":4, \"ALQ\":5, \"GLQ\":6}\nord_map2 = {\"NA\":0, \"No\":1, \"Mn\":2, \"Av\":3, \"Gd\":4}\n\nfor col in ordinal_feat:\n        \n    if len(df[col].unique()) <= 6 and col !=\"BsmtExposure\":\n        df[col] = df[col].map(ord_map)\n        df[col] = df[col].astype(int)\n        \n    elif col in ['OverallQual', 'OverallCond']:\n        df[col] = df[col].astype(int)\n        \n    elif df[col].name in ['BsmtFinType1', 'BsmtFinType2']:\n        df[col] = df[col].map(ord_map1)\n        df[col] = df[col].astype(int)\n        \n    else:\n        df[col] = df[col].map(ord_map2) \n        df[col] = df[col].astype(int)\n        \nfor col in ordinal_feat:\n    # Append nominal features to the numerical features\n    num_feat.append(col)\n    print(f\" Column '{col}' has unique values :{df[col].unique()}, dtype: {df[col].dtypes}\")\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let's remove some categorical columns we do not need anymore\nfor col in to_remove:\n    df.drop(col, axis=1, inplace=True)\n    cat_feat.remove(col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"####  Check numerical features correlation again"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(14,12))\n\nsns.heatmap(df[num_feat].corr(), mask= df[num_feat].corr()  < 0.8 , cbar=False, cmap='coolwarm', annot=True);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I introduce correlation between features when I converted some ordinal features and I need to remove one of the correlated feature. There is also high correlation between \"SalePrice\" and \"OverallQual\" (0.817185) but this is all right."},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_to_remove = ['GarageCond', 'FireplaceQu', 'BsmtFinType1','BsmtFinType2','BsmtCond',]\n\nfor col in corr_to_remove:\n    df = df.drop(col, axis=1)\n    num_feat.remove(col)\n    ordinal_feat.remove(col)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Nominal values**"},{"metadata":{},"cell_type":"markdown","source":"Different methods can be apply to convert nominal variables into numbers so I future algorithm can work with them. All of them have prons and cons but I am not going to write about it here. One of the simpliest and easy to understand is pandas \"get_dummies\" method, however you need to remember not to indroduce nulticollinearity what is also called (dummy trap). "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create dummy variables \ndummy_df = pd.get_dummies(df[cat_feat], drop_first=True)\n\nfor col in cat_feat:\n    df.drop(col, axis=1, inplace=True)\n    \ndf_with_dummies = pd.concat([df, dummy_df], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Second method for converting nominal categorical variables is OneHotEncode, but I am not going to use it in this notebook. Another common method used by practitioners is Label Encoding which suits more with variables who have some sort of order. "},{"metadata":{},"cell_type":"markdown","source":"**Now, this is very important that we split dataset back into test and train dataset before we scale the data.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split dataframe into test/train dataset\nclean_train_df = df_with_dummies[df_with_dummies[\"SalePrice\"] > 0].copy()\nclean_test_df = df_with_dummies[df_with_dummies[\"SalePrice\"].isna()].copy()\n\n# Drop SalePrice column from test set\nclean_test_df.drop(\"SalePrice\", axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_test_df.shape, clean_train_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Skewness**"},{"metadata":{"trusted":true},"cell_type":"code","source":"skewed_features = clean_train_df[num_feat].skew().sort_values(ascending=False)\nskewed_features = skewed_features[skewed_features > 0.5]\nskewed_index = skewed_features.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12,4), dpi=120)\nskewed_features.sort_values(ascending=False).plot(kind='bar')\nplt.xticks(rotation=45)\nplt.xticks(horizontalalignment=\"right\")\nplt.title(\"Skewed Feature above 0.5 upper limit\")\nplt.tight_layout();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First of all I will remove highly skewed features from dataset and then trim off the rest of them by 0.5 limit. Acceptable values for skewness are between -0.5 and 0.5 while -2 and 2 for Kurtosis."},{"metadata":{"trusted":true},"cell_type":"code","source":"right_skewness_col = ['PoolQC', 'LowQualFinSF']\nfor col in right_skewness_col:\n    clean_train_df.drop(col, axis=1, inplace=True)\n    clean_test_df.drop(col, axis=1, inplace= True)\n    num_feat.remove(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skewed_index = skewed_index.drop(['PoolQC','LowQualFinSF'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in skewed_index:\n    q3 = np.quantile(clean_train_df[col], 0.75)\n    q1 = np.quantile(clean_train_df[col], 0.25)\n    iqr = q3 - q1\n    # Upper limit for outliers\n    upper_limit = q3 + (1.5*iqr)\n    col_limit =  clean_train_df[col].apply(lambda x: x <= upper_limit)\n    clean_train_df = clean_train_df[col_limit]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save id column for submission\nrow_id = pd.Series(clean_test_df[\"Id\"])\nclean_train_df = clean_train_df.drop(\"Id\", axis=1).astype(\"float64\")\nclean_test_df = clean_test_df.drop(\"Id\", axis=1).astype(\"float64\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Splitting and standarization of the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split training dataset into X/y first\nX = clean_train_df.drop([\"SalePrice\"], axis=1)\ny = clean_train_df['SalePrice']\n\n# Then, split it into training and validation sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We all know how important is to standarize/normalize dataset for our algorithms but there are exceptions depend on what algorithm we are going to use. In order to do it right we need to remember about few things. Some suggest that it only descrete variables should be standarized and definitely we have to fit and transform train set and then only transforming test set. The reason for that we are not creating what is known as data leakage."},{"metadata":{"trusted":true},"cell_type":"code","source":"num_feat.remove('SalePrice')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaled_Xtrain = X_train.copy()\nscaled_Xtest = X_test.copy()\n\nscaler = StandardScaler()\n\nscaled_Xtrain[num_feat] = scaler.fit_transform(scaled_Xtrain[num_feat])\nscaled_Xtest[num_feat] = scaler.transform(scaled_Xtest[num_feat])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_Xtrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating and testing our models\n\n\n### Models score baseline"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error,mean_squared_error, r2_score\n\n# Create function for model evaluation\ndef model_evaluation(algo,algoname):\n    \"\"\"\n    This function  fit and  evaluate \n    given algorithm. It takes 3 arguments:\n    \n    First: algorithm of a choice without parentheses.\n    Second: the name of a algorithm as a string.\n    \"\"\"\n\n    # Fit given model\n    algo.fit(scaled_Xtrain, y_train)\n    y_pred = algo.predict(scaled_Xtest)\n    \n    # Calculate metrics\n    mae = mean_absolute_error(y_test, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    \n    # R-squared \n    r2score = r2_score(y_test, y_pred)\n    \n    print(f\"**{algoname} Metrics**\")\n    print(f\"**MAE: {mae:}\")\n    print(f\"**RMSE: {rmse:}\")\n    print(f\"**R-squared: {r2score:.2f}%\")\n    \n    return mae, rmse, r2score, y_pred, algo","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great place to start for someone who ask what algorithm I shoud use is sklearn algorithm cheat-sheet https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html. Follow their recommendation I am going to choose first Stochastic Gradient Descent. Let's create Stochastic Gradient Descent first."},{"metadata":{},"cell_type":"markdown","source":"**SGDRegressor**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a base model\nfrom sklearn.linear_model import SGDRegressor\n\nsgd_base_model = SGDRegressor(random_state=101)\n\nsgd_base_mae, sgd_base_rmse, sgd_base_r2score, sgd_y_pred, _ = model_evaluation(sgd_base_model, \n                                                                                \"SGDRegressor\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_residuals(y_pred, algoname):\n    \"\"\"\n    Function plots probability and residuals plot\n    \"\"\"\n    residuals = pd.Series(y_test - y_pred, \n                          name=\"residuals\")\n    \n    fig, axes = plt.subplots(ncols=2, \n                             nrows=2, \n                             figsize=(14,4), \n                             dpi=120)\n    # Plot probability\n    sp.stats.probplot(residuals, plot=axes[0,0])\n    # Plot kde\n    sns.distplot(residuals, ax=axes[0,1], hist=False)\n    # Plot residuals\n    sns.scatterplot(x=y_test, y=residuals, ax=axes[1,0])\n    axes[1,0].axhline(y=0, c='red',ls='--')\n    # Plot distribution\n    sns.boxplot(residuals, ax=axes[1,1])\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_residuals(sgd_y_pred, \"SGDRegressor\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Gradient Boosting Regressor**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\n\ngbr_model = GradientBoostingRegressor()\ngbr_base_mae, gbr_base_rmse, gbr_base_r2score, gbr_y_pred, gbr_model = model_evaluation(gbr_model, \n                                                                                        \"GradientBostingRegressor\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_residuals(gbr_y_pred, \"Gradient Boosting Regressor\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Random Forest Regressor**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nrfr_model = RandomForestRegressor()\nrfr_base_mae, rfr_base_rmse, rfr_base_r2score, rfr_y_pred, rfr_model = model_evaluation(rfr_model, \n                                                                                        \"RandomForestRegressor\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_residuals(rfr_y_pred, \"Random Forest Regressor\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Extreme Gradient Boosting**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\n\nxgboost_model = XGBRegressor()\nxgboost_base_mae, xgboost_base_rmse, xgboost_base_r2score, xgboost_y_pred, xgboost_model = model_evaluation(xgboost_model, \n                                                                                                            \"Extreme Gradient Boosting\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_residuals(xgboost_y_pred, \"Extreme Gradient Boosting\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**KNeighbors**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsRegressor\n\nknn_model = KNeighborsRegressor()\nknn_base_mae, knn_base_rmse, knn_base_r2score, knn_y_pred, knn_model = model_evaluation(knn_model, \n                                                                                        \"KNeighborsRegressor\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_residuals(knn_y_pred, \"KNeighborsRegressor\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Submmit to Kaggle with the best score to Kaggle competition**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Instantiate StandardScaler and copy dataset\nsc = StandardScaler()\nscaled_X = X.copy()\nscaled_test = clean_test_df.copy()\n\n# Scale the data\nscaled_X[num_feat] = sc.fit_transform(X[num_feat])\nscaled_test[num_feat] = sc.transform(clean_test_df[num_feat])\n\n# Instantiate the final model\n# final_base_model = GradientBoostingRegressor()\n\n# Fit the model\n# final_base_model.fit(scaled_X, y)\n\n# final_predictions = final_base_model.predict(scaled_test)\n\n\n# Make predictions and save it to the dataframe\n# final_base_model_df = pd.DataFrame({\"id\":row_id,\n                                    # \"SalePrice\": np.expm1(final_predictions)})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# final_base_model_df.to_csv(\"house_price_final_base_sub.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**ElasticNetCV**"},{"metadata":{},"cell_type":"markdown","source":"I am going to use ElasticNetCV in the base line models predictions as it will allow me to choose between Ridge(L2 regularization) or Lasso (L1 regularization). The benefit is that elastic net allows a balance of both penalties, which can result in better performance than a model with either one or the other penalty on some problems."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import ElasticNetCV\n\nelastic_model = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1])\n\nel_base_mae, el_base_rmse, el_base_r2score, el_base_y_pred, elastic_model = model_evaluation(elastic_model,\n                                                                                             \"ElasticNetCV\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_residuals(el_base_y_pred, \"ElasticNetCV\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"elastic_model.l1_ratio_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LassoCV\n\nlasso_cv_model = LassoCV(eps=0.01, n_alphas=200, cv=10, max_iter=1000000)\n\n\nlassoCV_mae, lassoCV_rmse, lassoCV_r2score, lassoCV_y_pred, lasso_cv_model = model_evaluation(lasso_cv_model, \"LassoCV\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_residuals(lassoCV_y_pred, \"LassoCV\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**RidgeCV**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import RidgeCV\n\nridge_model = RidgeCV(alphas=[0.1, 1.0, 10.0])\nridge_cv_mae, ridge_cv_rmse, ridge_cv_r2, ridge_cv_y_pred, ridge_model = model_evaluation(ridge_model,\n                                                                                          \"RidgeCV\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_model.alpha_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_residuals(ridge_cv_y_pred, \"RidgeCV\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVR\n\nsvr_base_model = SVR()\n\nsvr_base_mae, svr_base_rmse, svr_base_r2score, svr_base_y_red, svr_base_model = model_evaluation(svr_base_model, \n                                                                                                 \"Support Vector Regressor\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_residuals(svr_base_y_red, \"SVR\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**CatBoostRegressor**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from catboost import CatBoostRegressor\n\ncat_base = CatBoostRegressor(verbose=0, random_state=101)\n\ncat_base_mae, cat_base_rmse, cat_base_r2, cat_base_y_pred, cat_base_model = model_evaluation(cat_base,\n                                                                                             \"CatBoostRegressor\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_imp = cat_base.get_feature_importance(prettified=True)\n\n# Plotting top 20 features' importance\n\nplt.figure(figsize = (12,8))\nsns.barplot(feat_imp['Importances'][:20],feat_imp['Feature Id'][:20], orient = 'h', palette=\"coolwarm_r\")\nplt.title(\"Feature Importance\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import shap\nfrom catboost import Pool\n\n# Feature importance Interactive Plot \n\ntrain_pool = Pool(scaled_Xtrain)\nval_pool = Pool(scaled_Xtest)\n\nexplainer = shap.TreeExplainer(cat_base_model) # insert your model\nshap_values = explainer.shap_values(train_pool) # insert your train Pool object\n\nshap.summary_plot(shap_values, scaled_Xtrain)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Base model scores metrics"},{"metadata":{"trusted":true},"cell_type":"code","source":"base_score_df = pd.DataFrame({\"Model\":[\"SGDRegressor\", \"GradientBoostingRegressor\",\n                                       \"RandomForestRegressor\", \"Extreme Gradient Boosting\",\n                                       \"KNeighborsRegressor\" , \"LassoCV\", \"SVR\", \"RidgeCV\",\n                                       \"CatBoost\"],\n                              \n                              \"R-square\":[sgd_base_r2score, gbr_base_r2score, rfr_base_r2score,\n                                         xgboost_base_r2score, knn_base_r2score, lassoCV_r2score,\n                                         svr_base_r2score, ridge_cv_r2, cat_base_r2],\n                              \n                              \"RMSE\":[sgd_base_rmse, gbr_base_rmse, rfr_base_rmse, xgboost_base_rmse,\n                                      knn_base_rmse, lassoCV_rmse, svr_base_rmse, ridge_cv_rmse,\n                                      cat_base_rmse],\n                              \n                              \"MAE\": [sgd_base_mae, gbr_base_mae, rfr_base_mae, xgboost_base_mae,\n                                      knn_base_mae, lassoCV_mae, svr_base_mae, ridge_cv_mae,\n                                      cat_base_mae]})\n\nbase_score_df = base_score_df.sort_values(by=[\"R-square\"], \n                                          ascending=False).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"**Base Models Metrics**\")\nbase_score_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize the table above\nfig, ax = plt.subplots(figsize=(8,5))\n\nsns.barplot(x=\"Model\", y=\"R-square\", data=base_score_df, ax=ax, palette=\"magma\")\nsns.lineplot(x=\"Model\", y=\"RMSE\", data=base_score_df, color=\"red\", ax=ax,legend='brief', label=\"rmse\")\nsns.lineplot(x=\"Model\", y=\"MAE\", data=base_score_df, color='green', ax=ax, legend='brief', label=\"mae\")\n\nplt.xticks(rotation=45, horizontalalignment=\"right\")\nplt.title(\"Regression Model Performance Metrics\")\nplt.ylabel(\"R_squared\")\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Submmit Voting Ensamble Model with base models"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingRegressor\n\nensemble1_model = VotingRegressor(estimators=[(\"ridgecv\", ridge_model),\n                                             (\"catboost\", cat_base_model),\n                                             (\"gbr\", gbr_model),\n                                             (\"lassocv\", lasso_cv_model),\n                                             (\"svr\", svr_base_model),\n                                             (\"forest\", rfr_model)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble1_mae, ensemble1_rmse, ensemble1_r2, _ , ensemble1_model = model_evaluation(ensemble1_model,\n                                                                                    \"Voting Regressor\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Submmit ensemble model to the competition**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the model\n# ensemble_model1.fit(scaled_X, y)\n\n# final_ensemble = ensemble_model1.predict(scaled_test)\n\n\n# Make predictions and save it to the dataframe\n# final_base_ensemble_df = pd.DataFrame({\"id\":row_id,\n                                       # \"SalePrice\": np.expm1(final_ensemble)})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# final_base_ensemble_df.to_csv(\"house_price_final_ensemble_base_sub.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### GridSearchCV for the best hyperparameters"},{"metadata":{},"cell_type":"markdown","source":"GridSearchCV is a exhaustive search over specified hyperparameters values for an estimator. It allow us to find the best combination of best parameters for a chosen model. I will split data again with test size of 0.1."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\ndef model_gridsearchCV(algo,param,name):\n    \"\"\"\n    Function will perform gridsearchCV for given algorithm\n    and parameter grid. Returns grid model, y_pred. Prints out \n    mean absolute error, root mean squared error, R-square score\n    \"\"\"\n    # Instatiate base model\n    model = algo()\n    \n    # Instantiate grid for a model\n    model_grid = GridSearchCV(model, \n                             param,\n                             scoring=\"r2\",\n                             # verbose=2,\n                             n_jobs=-1,\n                             cv=3)\n    # Fit the grid model\n    model_grid.fit(scaled_Xtrain, y_train)\n    \n    # Make prediction\n    y_pred = model_grid.predict(scaled_Xtest)\n    \n    # Evaluate model\n    mae = mean_absolute_error(y_test, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    r2score = r2_score(y_test, y_pred)\n    \n    # Print \n    print(f\"**{name} with GridSearchCV**\")\n    print(f\"MAE: {mae:}\")\n    print(f\"RMSE: {rmse:}\")\n    print(f\"R-squared: {r2score:.2f}%\")\n    \n    return mae, rmse, r2score, y_pred, model_grid","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**GradientBoostingRegressor**"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {#\"loss\":[\"ls\",\"lad\",\"huber\",\"quantile\"],\n              \"learning_rate\": [ 0.01, 0.1, 0.3, 1],\n              \"subsample\": [0.5, 0.2, 0.1],\n              \"n_estimators\": [500, 1000],\n              \"max_depth\": [3,6,8]}\n\ngbr_grid_mae, gbr_grid_rmse, gbr_grid_r2, _ , gbr_grid = model_gridsearchCV(GradientBoostingRegressor, \n                                                                            param_grid,\n                                                                            \"GradientBoostingRegressor\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gbr_grid.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Random Forest Regressor**"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {\"n_estimators\": [500,1000, 1500],\n              \"max_features\": ['auto','sqrt'],\n              \"max_depth\": [None,5,10,],\n              \"min_samples_split\": [2,5,10],\n              \"min_samples_leaf\": [1,2,5,10]}\n\nrfr_grid_mae, rfr_grid_rmse, rfr_grid_r2, _ , rfr_grid_model = model_gridsearchCV(RandomForestRegressor,\n                                                                                  param_grid,\n                                                                                  \"RandomForestRegressor\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfr_grid_model.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**SVR**"},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {\"kernel\":[\"linear\",\"rbf\",],\n              \"gamma\": [\"scale\",\"auto\"],\n              \"C\": [0.1, 0.5, 1, 10],\n              \"epsilon\": [0.1, 0.01, 0.001]}\n\nsvr_grid_mae, svr_grid_rmse, svr_grid_r2, svr_grid_y_pred, svr_grid_model = model_gridsearchCV(SVR,\n                                                                                param_grid,\n                                                                               \"SVR\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svr_grid_model.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"svr_grid_model.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Ridge**\n\nRidgeCV had the best metrics and I want to see if GridSearchCV can improve the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge\n\nparam_grid = {\"solver\": [\"auto\",\"svd\",\"lsqr\",\"saga\"],\n              \"max_iter\": [1000, 10000],\n              \"tol\": [1e-3,1e-2],\n              \"alpha\": [0.1, 1.0, 10.0, 30.0]}\n\nridge_gr_mae, ridge_gr_rmse, ridge_gr_r2,_ , ridge_gr_model = model_gridsearchCV(Ridge,\n                                                                                 param_grid,\n                                                                                 \"Ridge\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ridge_gr_model.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Extreme Gradient Boosting**"},{"metadata":{},"cell_type":"markdown","source":"There is a warning when running this algoritm, but It should not prevent your code from running, nor should it lead to different results."},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {\"learning_rate\":[0.05, 0.10, 0.15, 0.20, 0.30],\n              \"max_depth\":[3,4,5,6,8,15],\n              \"min_child_weight\":[1,3,5,7],\n              \"gamma\":[0.0, 0.1, 0.2, 0.3, 0.4],\n              \"colsample_bytree\":[0.3, 0.4, 0.5, 0.7]}\n\nxboost_gr_mae, xboost_gr_rmse, xboost_gr_r2, _ , xboost_gr_model = model_gridsearchCV(XGBRegressor,\n                                                                                      param_grid,\n                                                                                      \"XGBoost\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xboost_gr_model.best_params_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**CatBoostRegressor**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#param_grid = {'iterations': [250,100,500,1000],\n              #'learning_rate': [0.01,0.1,0.2,0.3],\n              #'depth': [4, 6],\n              #'l2_leaf_reg': [3,1,5,10,100]}\n\n\n# cat_grid_mae, cat_grid_rmse, cat_grid_r2, _ , cat_grid_model = model_gridsearchCV(CatBoostRegressor,\n                                                                                  # param_grid,\n                                                                                  # \"CatBoost\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**CatBoost with GridSearchCV**\n1. MAE: 0.08686650731538992\n2. RMSE: 0.12366782337837257\n3. R-squared: 0.92%"},{"metadata":{"trusted":true},"cell_type":"code","source":"cat_grid_model.best_params_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grCV_metrics_df = pd.DataFrame({\"Model\":[\"GradientBoostingRegressor\", \"RandomForestRegressor\", \n                                         \"SVR\", \"Ridge\", \"XGBRegressor\", \"CatBoost\"],\n                                        \n                                \"R-square\":[gbr_grid_r2, rfr_grid_r2, svr_grid_r2, \n                                            ridge_gr_r2, xboost_gr_r2, cat_grid_r2],\n                                        \n                                \"RMSE\":[gbr_grid_rmse, rfr_grid_rmse, svr_grid_rmse, \n                                        ridge_gr_rmse, xboost_gr_rmse, cat_grid_rmse],\n                                        \n                                \"MAE\":[gbr_grid_mae, rfr_grid_mae, svr_grid_mae, \n                                      ridge_gr_mae, xboost_gr_mae, cat_grid_mae]})\n\ngrCV_mertics_df = grCV_metrics_df.sort_values(by=[\"R-square\"],\n                                              ascending=False).reset_index(drop=True)\n\nprint(\"**GridSearchCV Models Metrics**\")\ngrCV_mertics_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Visualize the table above\nfig, ax = plt.subplots(figsize=(8,5))\n\nlist_order = list(grCV_mertics_df['Model'].values)\n# R-squared\nsns.barplot(x=\"Model\", y=\"R-square\", \n            data=grCV_metrics_df, ax=ax, \n            palette=\"magma\", order= list_order)\n# Root Mean Squared Error\nsns.lineplot(x=\"Model\", y=\"RMSE\", data=grCV_metrics_df, \n             color=\"red\", ax=ax,legend='brief', label=\"rmse\")\n# Mean Absolute Error\nsns.lineplot(x=\"Model\", y=\"MAE\", data=grCV_metrics_df, \n             color='green', ax=ax, legend='brief', label=\"mae\")\n\nplt.xticks(rotation=45, horizontalalignment=\"right\")\nplt.title(\"Regression Models with GridSearchCV Metrics\")\nplt.ylabel(\"R_squared\")\nplt.legend();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Ensemble model with best parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble2_model = VotingRegressor(estimators=[(\"ridgecv\", ridge_gr_model.estimator),\n                                             (\"catboost\", cat_grid_model.estimator),\n                                             (\"gbr\", gbr_grid.estimator),\n                                             (\"lassocv\", lasso_cv_model),\n                                             (\"svr\", svr_base_model),\n                                             (\"forest\", rfr_model.base_estimator)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit the model\n# ensemble2_model.fit(scaled_Xtrain, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate ensemble model\nensemble2_y_pred = ensemble2_model.predict(scaled_Xtest)\n\nensemble2_mae = mean_absolute_error(y_test, ensemble2_y_pred)\nensemble2_rmse = np.sqrt(mean_squared_error(y_test, ensemble2_y_pred))\n    \n# R-squared \nensemble2_r2 = r2_score(y_test, ensemble2_y_pred)\n    \nprint(f\"**VotingRegressor Metrics**\")\nprint(f\"**MAE: {ensemble2_mae}\")\nprint(f\"**RMSE: {ensemble2_rmse}\")\nprint(f\"**R-squared: {ensemble2_r2:.2f}%\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Make predictions submmision to Kaggle"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_ensemble = VotingRegressor(estimators=[(\"gbr\", gbr_grid.estimator),\n                                            (\"forest\", rfr_grid_model.estimator),\n                                            (\"svr\", svr_grid_model.estimator),\n                                            (\"ridge\", ridge_gr_model.estimator),\n                                            (\"xgboost\", xboost_gr_model.estimator),\n                                            (\"catboost\", cat_grid_model.estimator)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#fit the model\n#best_ensemble.fit(scaled_X, y)\n\n#final_ensemble2 = best_ensemble.predict(scaled_test)\n\n\n#Make predictions and save it to the dataframe\n#final_ensemble_df = pd.DataFrame({\"id\":row_id,\"SalePrice\": np.expm1(final_ensemble2)})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_ensemble_df.to_csv(\"house_price_grid_ensemble_sub.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I have to admit that spent all that time on testing didn't help to improve the model in this case .After submmision to Kaggle competition I end up in top 8%. Now, I need to figure it out what to do next or which techniques I could implement to make this model even more robust. I've got some ideas already in my mind, anyway, back to reading and searching. So, if you Kagglers have some ideas let me know. Don't forget to leave feadback."}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}